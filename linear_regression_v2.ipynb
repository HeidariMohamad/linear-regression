{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "769397db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2585c483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System stats\n",
      "2.7.1+cu128\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"System stats\")\n",
    "print(torch.__version__)\n",
    "device = \"cuda\" if(torch.cuda.is_available()) else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2935c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data\n",
    "weight=.3\n",
    "bias=.8\n",
    "X=torch.arange(1,1000,10).unsqueeze(dim=1).float()\n",
    "y=X*weight+bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2ab339da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset stats\n",
      " Number of train: 80 \n",
      " Number of test 20 \n",
      " Total: 100\n"
     ]
    }
   ],
   "source": [
    "#split training and testing data\n",
    "x_train,y_train=X[:int(len(X)*.8)],y[:int(len(X)*.8)]\n",
    "x_test,y_test=X[int(len(X)*.8):],y[int(len(X)*.8):]\n",
    "print(f\"dataset stats\\n Number of train: {len(x_train)} \\n Number of test {len(x_test)} \\n Total: {len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6acf859c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x74ded80982d0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.scatter(x_train,y_train,c=\"blue\",s=4,label=\"train\")\n",
    "plt.scatter(x_test,y_test,c=\"orange\",s=4,label=\"test\")\n",
    "plt.legend(prop={\"size\":15})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "367c967f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(linearlayermodel(\n",
       "   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
       " ),\n",
       " OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n",
       "              ('linear_layer.bias', tensor([0.8300]))]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class linearlayermodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer=nn.Linear(in_features=1,\n",
    "           \n",
    "                               out_features=1)\n",
    "    def forward(self,X:torch.Tensor):\n",
    "        return self.linear_layer(X)\n",
    "\n",
    "model_1=linearlayermodel()\n",
    "model_1,model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2aaac77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(list(model_1.parameters())[0].device)\n",
    "\n",
    "print(next(model_1.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8cb107d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.to(device)\n",
    "next(model_1.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "891c0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.000001)\n",
    "#loss_fn=nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a2ecd2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x74dfa6929990>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39ea59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train loss: 0.13620 | Test loss: 0.01352\n",
      "Epoch: 10 | Train loss: 0.10205 | Test loss: 0.08877\n",
      "Epoch: 20 | Train loss: 0.11761 | Test loss: 0.05464\n",
      "Epoch: 30 | Train loss: 0.13027 | Test loss: 0.02714\n",
      "Epoch: 40 | Train loss: 0.14086 | Test loss: 0.00283\n",
      "Epoch: 50 | Train loss: 0.09732 | Test loss: 0.09971\n",
      "Epoch: 60 | Train loss: 0.11366 | Test loss: 0.06377\n",
      "Epoch: 70 | Train loss: 0.12769 | Test loss: 0.03306\n",
      "Epoch: 80 | Train loss: 0.13828 | Test loss: 0.00876\n",
      "Epoch: 90 | Train loss: 0.10558 | Test loss: 0.08059\n",
      "Epoch: 100 | Train loss: 0.12038 | Test loss: 0.04828\n",
      "Epoch: 110 | Train loss: 0.13235 | Test loss: 0.02237\n",
      "Epoch: 120 | Train loss: 0.14294 | Test loss: 0.00219\n",
      "Epoch: 130 | Train loss: 0.10086 | Test loss: 0.09153\n",
      "Epoch: 140 | Train loss: 0.11642 | Test loss: 0.05740\n",
      "Epoch: 150 | Train loss: 0.12976 | Test loss: 0.02830\n",
      "Epoch: 160 | Train loss: 0.14036 | Test loss: 0.00400\n",
      "Epoch: 170 | Train loss: 0.10912 | Test loss: 0.07422\n",
      "Epoch: 180 | Train loss: 0.12314 | Test loss: 0.04191\n",
      "Epoch: 190 | Train loss: 0.13442 | Test loss: 0.01761\n",
      "Epoch: 200 | Train loss: 0.14502 | Test loss: 0.00669\n",
      "Epoch: 210 | Train loss: 0.10439 | Test loss: 0.08335\n",
      "Epoch: 220 | Train loss: 0.11918 | Test loss: 0.05104\n",
      "Epoch: 230 | Train loss: 0.13184 | Test loss: 0.02354\n",
      "Epoch: 240 | Train loss: 0.14244 | Test loss: 0.00163\n",
      "Epoch: 250 | Train loss: 0.11470 | Test loss: 0.06137\n",
      "Epoch: 260 | Train loss: 0.12804 | Test loss: 0.03226\n",
      "Epoch: 270 | Train loss: 0.13863 | Test loss: 0.00796\n",
      "Epoch: 280 | Train loss: 0.09277 | Test loss: 0.10826\n",
      "Epoch: 290 | Train loss: 0.11074 | Test loss: 0.07050\n",
      "Epoch: 300 | Train loss: 0.12476 | Test loss: 0.03819\n",
      "Epoch: 310 | Train loss: 0.13605 | Test loss: 0.01389\n",
      "Epoch: 320 | Train loss: 0.10908 | Test loss: 0.07434\n",
      "Epoch: 330 | Train loss: 0.12310 | Test loss: 0.04203\n",
      "Epoch: 340 | Train loss: 0.13438 | Test loss: 0.01773\n",
      "Epoch: 350 | Train loss: 0.14497 | Test loss: 0.00657\n",
      "Epoch: 360 | Train loss: 0.10434 | Test loss: 0.08347\n",
      "Epoch: 370 | Train loss: 0.11914 | Test loss: 0.05116\n",
      "Epoch: 380 | Train loss: 0.13180 | Test loss: 0.02365\n",
      "Epoch: 390 | Train loss: 0.14239 | Test loss: 0.00160\n",
      "Epoch: 400 | Train loss: 0.11747 | Test loss: 0.05500\n",
      "Epoch: 410 | Train loss: 0.13012 | Test loss: 0.02750\n",
      "Epoch: 420 | Train loss: 0.14072 | Test loss: 0.00320\n",
      "Epoch: 430 | Train loss: 0.09980 | Test loss: 0.09401\n",
      "Epoch: 440 | Train loss: 0.11614 | Test loss: 0.05807\n",
      "Epoch: 450 | Train loss: 0.12949 | Test loss: 0.02896\n",
      "Epoch: 460 | Train loss: 0.14008 | Test loss: 0.00466\n",
      "Epoch: 470 | Train loss: 0.12178 | Test loss: 0.04509\n",
      "Epoch: 480 | Train loss: 0.13375 | Test loss: 0.01918\n",
      "Epoch: 490 | Train loss: 0.14434 | Test loss: 0.00512\n",
      "Epoch: 500 | Train loss: 0.12742 | Test loss: 0.03371\n",
      "Epoch: 510 | Train loss: 0.13801 | Test loss: 0.00941\n",
      "Epoch: 520 | Train loss: 0.11902 | Test loss: 0.05144\n",
      "Epoch: 530 | Train loss: 0.13168 | Test loss: 0.02393\n",
      "Epoch: 540 | Train loss: 0.14227 | Test loss: 0.00154\n",
      "Epoch: 550 | Train loss: 0.11736 | Test loss: 0.05528\n",
      "Epoch: 560 | Train loss: 0.13001 | Test loss: 0.02778\n",
      "Epoch: 570 | Train loss: 0.14060 | Test loss: 0.00348\n",
      "Epoch: 580 | Train loss: 0.10229 | Test loss: 0.08826\n",
      "Epoch: 590 | Train loss: 0.11786 | Test loss: 0.05414\n",
      "Epoch: 600 | Train loss: 0.13051 | Test loss: 0.02663\n",
      "Epoch: 610 | Train loss: 0.14110 | Test loss: 0.00236\n",
      "Epoch: 620 | Train loss: 0.09168 | Test loss: 0.11082\n",
      "Epoch: 630 | Train loss: 0.11043 | Test loss: 0.07124\n",
      "Epoch: 640 | Train loss: 0.12445 | Test loss: 0.03893\n",
      "Epoch: 650 | Train loss: 0.13574 | Test loss: 0.01463\n",
      "Epoch: 660 | Train loss: 0.11607 | Test loss: 0.05826\n",
      "Epoch: 670 | Train loss: 0.12941 | Test loss: 0.02915\n",
      "Epoch: 680 | Train loss: 0.14000 | Test loss: 0.00485\n",
      "Epoch: 690 | Train loss: 0.12170 | Test loss: 0.04528\n",
      "Epoch: 700 | Train loss: 0.13367 | Test loss: 0.01938\n",
      "Epoch: 710 | Train loss: 0.14427 | Test loss: 0.00492\n",
      "Epoch: 720 | Train loss: 0.12734 | Test loss: 0.03390\n",
      "Epoch: 730 | Train loss: 0.13794 | Test loss: 0.00960\n",
      "Epoch: 740 | Train loss: 0.11895 | Test loss: 0.05163\n",
      "Epoch: 750 | Train loss: 0.13161 | Test loss: 0.02412\n",
      "Epoch: 760 | Train loss: 0.14220 | Test loss: 0.00151\n",
      "Epoch: 770 | Train loss: 0.12458 | Test loss: 0.03865\n",
      "Epoch: 780 | Train loss: 0.13587 | Test loss: 0.01435\n",
      "Epoch: 790 | Train loss: 0.11619 | Test loss: 0.05798\n",
      "Epoch: 800 | Train loss: 0.12954 | Test loss: 0.02888\n",
      "Epoch: 810 | Train loss: 0.14013 | Test loss: 0.00457\n",
      "Epoch: 820 | Train loss: 0.12183 | Test loss: 0.04500\n",
      "Epoch: 830 | Train loss: 0.13380 | Test loss: 0.01910\n",
      "Epoch: 840 | Train loss: 0.14439 | Test loss: 0.00520\n",
      "Epoch: 850 | Train loss: 0.12016 | Test loss: 0.04884\n",
      "Epoch: 860 | Train loss: 0.13213 | Test loss: 0.02294\n",
      "Epoch: 870 | Train loss: 0.14272 | Test loss: 0.00186\n",
      "Epoch: 880 | Train loss: 0.10586 | Test loss: 0.08000\n",
      "Epoch: 890 | Train loss: 0.12066 | Test loss: 0.04769\n",
      "Epoch: 900 | Train loss: 0.13263 | Test loss: 0.02179\n",
      "Epoch: 910 | Train loss: 0.14322 | Test loss: 0.00260\n",
      "Epoch: 920 | Train loss: 0.09612 | Test loss: 0.10256\n",
      "Epoch: 930 | Train loss: 0.11324 | Test loss: 0.06481\n",
      "Epoch: 940 | Train loss: 0.12727 | Test loss: 0.03410\n",
      "Epoch: 950 | Train loss: 0.13786 | Test loss: 0.00979\n",
      "Epoch: 960 | Train loss: 0.11887 | Test loss: 0.05182\n",
      "Epoch: 970 | Train loss: 0.13153 | Test loss: 0.02432\n",
      "Epoch: 980 | Train loss: 0.14212 | Test loss: 0.00149\n",
      "Epoch: 990 | Train loss: 0.12450 | Test loss: 0.03884\n",
      "Epoch: 1000 | Train loss: 0.13579 | Test loss: 0.01454\n",
      "Epoch: 1010 | Train loss: 0.11612 | Test loss: 0.05817\n",
      "Epoch: 1020 | Train loss: 0.12946 | Test loss: 0.02907\n",
      "Epoch: 1030 | Train loss: 0.14006 | Test loss: 0.00476\n",
      "Epoch: 1040 | Train loss: 0.12175 | Test loss: 0.04519\n",
      "Epoch: 1050 | Train loss: 0.13372 | Test loss: 0.01929\n",
      "Epoch: 1060 | Train loss: 0.14432 | Test loss: 0.00501\n",
      "Epoch: 1070 | Train loss: 0.12739 | Test loss: 0.03381\n",
      "Epoch: 1080 | Train loss: 0.13799 | Test loss: 0.00952\n",
      "Epoch: 1090 | Train loss: 0.11900 | Test loss: 0.05154\n",
      "Epoch: 1100 | Train loss: 0.13166 | Test loss: 0.02403\n",
      "Epoch: 1110 | Train loss: 0.14225 | Test loss: 0.00152\n",
      "Epoch: 1120 | Train loss: 0.12463 | Test loss: 0.03856\n",
      "Epoch: 1130 | Train loss: 0.13592 | Test loss: 0.01426\n",
      "Epoch: 1140 | Train loss: 0.11259 | Test loss: 0.06631\n",
      "Epoch: 1150 | Train loss: 0.12662 | Test loss: 0.03560\n",
      "Epoch: 1160 | Train loss: 0.13721 | Test loss: 0.01130\n",
      "Epoch: 1170 | Train loss: 0.06893 | Test loss: 0.15939\n",
      "Epoch: 1180 | Train loss: 0.09493 | Test loss: 0.10534\n",
      "Epoch: 1190 | Train loss: 0.11204 | Test loss: 0.06758\n",
      "Epoch: 1200 | Train loss: 0.12607 | Test loss: 0.03687\n",
      "Epoch: 1210 | Train loss: 0.13666 | Test loss: 0.01257\n",
      "Epoch: 1220 | Train loss: 0.08848 | Test loss: 0.11832\n",
      "Epoch: 1230 | Train loss: 0.10808 | Test loss: 0.07671\n",
      "Epoch: 1240 | Train loss: 0.12210 | Test loss: 0.04440\n",
      "Epoch: 1250 | Train loss: 0.13408 | Test loss: 0.01850\n",
      "Epoch: 1260 | Train loss: 0.14467 | Test loss: 0.00580\n",
      "Epoch: 1270 | Train loss: 0.11480 | Test loss: 0.06122\n",
      "Epoch: 1280 | Train loss: 0.12814 | Test loss: 0.03211\n",
      "Epoch: 1290 | Train loss: 0.13874 | Test loss: 0.00781\n",
      "Epoch: 1300 | Train loss: 0.09287 | Test loss: 0.10811\n",
      "Epoch: 1310 | Train loss: 0.11085 | Test loss: 0.07035\n",
      "Epoch: 1320 | Train loss: 0.12487 | Test loss: 0.03804\n",
      "Epoch: 1330 | Train loss: 0.13616 | Test loss: 0.01374\n",
      "Epoch: 1340 | Train loss: 0.10200 | Test loss: 0.08898\n",
      "Epoch: 1350 | Train loss: 0.11757 | Test loss: 0.05486\n",
      "Epoch: 1360 | Train loss: 0.13022 | Test loss: 0.02735\n",
      "Epoch: 1370 | Train loss: 0.14082 | Test loss: 0.00305\n",
      "Epoch: 1380 | Train loss: 0.09728 | Test loss: 0.09993\n",
      "Epoch: 1390 | Train loss: 0.11361 | Test loss: 0.06398\n",
      "Epoch: 1400 | Train loss: 0.12764 | Test loss: 0.03327\n",
      "Epoch: 1410 | Train loss: 0.13824 | Test loss: 0.00897\n",
      "Epoch: 1420 | Train loss: 0.10553 | Test loss: 0.08081\n",
      "Epoch: 1430 | Train loss: 0.12033 | Test loss: 0.04850\n",
      "Epoch: 1440 | Train loss: 0.13230 | Test loss: 0.02259\n",
      "Epoch: 1450 | Train loss: 0.14289 | Test loss: 0.00205\n",
      "Epoch: 1460 | Train loss: 0.10081 | Test loss: 0.09175\n",
      "Epoch: 1470 | Train loss: 0.11637 | Test loss: 0.05762\n",
      "Epoch: 1480 | Train loss: 0.12972 | Test loss: 0.02851\n",
      "Epoch: 1490 | Train loss: 0.14031 | Test loss: 0.00421\n",
      "Epoch: 1500 | Train loss: 0.11471 | Test loss: 0.06146\n",
      "Epoch: 1510 | Train loss: 0.12805 | Test loss: 0.03236\n",
      "Epoch: 1520 | Train loss: 0.13864 | Test loss: 0.00806\n",
      "Epoch: 1530 | Train loss: 0.09277 | Test loss: 0.10835\n",
      "Epoch: 1540 | Train loss: 0.11075 | Test loss: 0.07059\n",
      "Epoch: 1550 | Train loss: 0.12477 | Test loss: 0.03828\n",
      "Epoch: 1560 | Train loss: 0.13606 | Test loss: 0.01398\n",
      "Epoch: 1570 | Train loss: 0.10908 | Test loss: 0.07443\n",
      "Epoch: 1580 | Train loss: 0.12310 | Test loss: 0.04212\n",
      "Epoch: 1590 | Train loss: 0.13439 | Test loss: 0.01782\n",
      "Epoch: 1600 | Train loss: 0.14498 | Test loss: 0.00648\n",
      "Epoch: 1610 | Train loss: 0.10696 | Test loss: 0.07932\n",
      "Epoch: 1620 | Train loss: 0.12099 | Test loss: 0.04700\n",
      "Epoch: 1630 | Train loss: 0.13296 | Test loss: 0.02110\n",
      "Epoch: 1640 | Train loss: 0.14355 | Test loss: 0.00320\n",
      "Epoch: 1650 | Train loss: 0.08605 | Test loss: 0.12401\n",
      "Epoch: 1660 | Train loss: 0.10563 | Test loss: 0.08059\n",
      "Epoch: 1670 | Train loss: 0.12043 | Test loss: 0.04828\n",
      "Epoch: 1680 | Train loss: 0.13240 | Test loss: 0.02238\n",
      "Epoch: 1690 | Train loss: 0.14300 | Test loss: 0.00218\n",
      "Epoch: 1700 | Train loss: 0.10003 | Test loss: 0.09357\n",
      "Epoch: 1710 | Train loss: 0.11559 | Test loss: 0.05944\n",
      "Epoch: 1720 | Train loss: 0.12894 | Test loss: 0.03033\n",
      "Epoch: 1730 | Train loss: 0.13953 | Test loss: 0.00603\n",
      "Epoch: 1740 | Train loss: 0.14239 | Test loss: 0.00157\n",
      "Epoch: 1750 | Train loss: 0.11747 | Test loss: 0.05511\n",
      "Epoch: 1760 | Train loss: 0.13013 | Test loss: 0.02760\n",
      "Epoch: 1770 | Train loss: 0.14072 | Test loss: 0.00331\n",
      "Epoch: 1780 | Train loss: 0.10240 | Test loss: 0.08808\n",
      "Epoch: 1790 | Train loss: 0.11797 | Test loss: 0.05396\n",
      "Epoch: 1800 | Train loss: 0.13063 | Test loss: 0.02645\n",
      "Epoch: 1810 | Train loss: 0.14122 | Test loss: 0.00222\n",
      "Epoch: 1820 | Train loss: 0.08603 | Test loss: 0.12408\n",
      "Epoch: 1830 | Train loss: 0.10561 | Test loss: 0.08066\n",
      "Epoch: 1840 | Train loss: 0.12041 | Test loss: 0.04834\n",
      "Epoch: 1850 | Train loss: 0.13238 | Test loss: 0.02244\n",
      "Epoch: 1860 | Train loss: 0.14298 | Test loss: 0.00214\n",
      "Epoch: 1870 | Train loss: 0.10001 | Test loss: 0.09363\n",
      "Epoch: 1880 | Train loss: 0.11557 | Test loss: 0.05951\n",
      "Epoch: 1890 | Train loss: 0.12892 | Test loss: 0.03039\n",
      "Epoch: 1900 | Train loss: 0.13951 | Test loss: 0.00609\n",
      "Epoch: 1910 | Train loss: 0.14237 | Test loss: 0.00155\n",
      "Epoch: 1920 | Train loss: 0.11745 | Test loss: 0.05517\n",
      "Epoch: 1930 | Train loss: 0.13011 | Test loss: 0.02767\n",
      "Epoch: 1940 | Train loss: 0.14070 | Test loss: 0.00337\n",
      "Epoch: 1950 | Train loss: 0.10238 | Test loss: 0.08815\n",
      "Epoch: 1960 | Train loss: 0.11795 | Test loss: 0.05402\n",
      "Epoch: 1970 | Train loss: 0.13061 | Test loss: 0.02652\n",
      "Epoch: 1980 | Train loss: 0.14120 | Test loss: 0.00226\n",
      "Epoch: 1990 | Train loss: 0.08601 | Test loss: 0.12414\n",
      "Epoch: 2000 | Train loss: 0.10559 | Test loss: 0.08072\n",
      "Epoch: 2010 | Train loss: 0.12039 | Test loss: 0.04841\n",
      "Epoch: 2020 | Train loss: 0.13236 | Test loss: 0.02251\n",
      "Epoch: 2030 | Train loss: 0.14295 | Test loss: 0.00209\n",
      "Epoch: 2040 | Train loss: 0.09999 | Test loss: 0.09369\n",
      "Epoch: 2050 | Train loss: 0.11555 | Test loss: 0.05957\n",
      "Epoch: 2060 | Train loss: 0.12890 | Test loss: 0.03046\n",
      "Epoch: 2070 | Train loss: 0.13949 | Test loss: 0.00616\n",
      "Epoch: 2080 | Train loss: 0.14235 | Test loss: 0.00154\n",
      "Epoch: 2090 | Train loss: 0.11743 | Test loss: 0.05524\n",
      "Epoch: 2100 | Train loss: 0.13009 | Test loss: 0.02773\n",
      "Epoch: 2110 | Train loss: 0.14068 | Test loss: 0.00343\n",
      "Epoch: 2120 | Train loss: 0.10236 | Test loss: 0.08821\n",
      "Epoch: 2130 | Train loss: 0.11793 | Test loss: 0.05408\n",
      "Epoch: 2140 | Train loss: 0.13059 | Test loss: 0.02658\n",
      "Epoch: 2150 | Train loss: 0.14118 | Test loss: 0.00231\n",
      "Epoch: 2160 | Train loss: 0.08599 | Test loss: 0.12420\n",
      "Epoch: 2170 | Train loss: 0.10557 | Test loss: 0.08078\n",
      "Epoch: 2180 | Train loss: 0.12037 | Test loss: 0.04847\n",
      "Epoch: 2190 | Train loss: 0.13234 | Test loss: 0.02257\n",
      "Epoch: 2200 | Train loss: 0.14293 | Test loss: 0.00205\n",
      "Epoch: 2210 | Train loss: 0.09997 | Test loss: 0.09376\n",
      "Epoch: 2220 | Train loss: 0.11553 | Test loss: 0.05963\n",
      "Epoch: 2230 | Train loss: 0.12887 | Test loss: 0.03052\n",
      "Epoch: 2240 | Train loss: 0.13947 | Test loss: 0.00622\n",
      "Epoch: 2250 | Train loss: 0.07545 | Test loss: 0.14660\n",
      "Epoch: 2260 | Train loss: 0.09863 | Test loss: 0.09685\n",
      "Epoch: 2270 | Train loss: 0.11498 | Test loss: 0.06091\n",
      "Epoch: 2280 | Train loss: 0.12832 | Test loss: 0.03180\n",
      "Epoch: 2290 | Train loss: 0.13891 | Test loss: 0.00749\n",
      "Epoch: 2300 | Train loss: 0.08620 | Test loss: 0.12371\n",
      "Epoch: 2310 | Train loss: 0.10578 | Test loss: 0.08029\n",
      "Epoch: 2320 | Train loss: 0.12059 | Test loss: 0.04798\n",
      "Epoch: 2330 | Train loss: 0.13256 | Test loss: 0.02207\n",
      "Epoch: 2340 | Train loss: 0.14315 | Test loss: 0.00238\n",
      "Epoch: 2350 | Train loss: 0.09507 | Test loss: 0.10512\n",
      "Epoch: 2360 | Train loss: 0.11218 | Test loss: 0.06736\n",
      "Epoch: 2370 | Train loss: 0.12621 | Test loss: 0.03665\n",
      "Epoch: 2380 | Train loss: 0.13680 | Test loss: 0.01235\n",
      "Epoch: 2390 | Train loss: 0.08179 | Test loss: 0.13401\n",
      "Epoch: 2400 | Train loss: 0.10222 | Test loss: 0.08856\n",
      "Epoch: 2410 | Train loss: 0.11779 | Test loss: 0.05443\n",
      "Epoch: 2420 | Train loss: 0.13045 | Test loss: 0.02692\n",
      "Epoch: 2430 | Train loss: 0.14104 | Test loss: 0.00263\n",
      "Epoch: 2440 | Train loss: 0.09064 | Test loss: 0.11339\n",
      "Epoch: 2450 | Train loss: 0.10938 | Test loss: 0.07381\n",
      "Epoch: 2460 | Train loss: 0.12341 | Test loss: 0.04150\n",
      "Epoch: 2470 | Train loss: 0.13469 | Test loss: 0.01720\n",
      "Epoch: 2480 | Train loss: 0.14528 | Test loss: 0.00710\n",
      "Epoch: 2490 | Train loss: 0.09865 | Test loss: 0.09683\n",
      "Epoch: 2500 | Train loss: 0.11500 | Test loss: 0.06088\n",
      "Epoch: 2510 | Train loss: 0.12834 | Test loss: 0.03178\n",
      "Epoch: 2520 | Train loss: 0.13893 | Test loss: 0.00747\n",
      "Epoch: 2530 | Train loss: 0.08622 | Test loss: 0.12369\n",
      "Epoch: 2540 | Train loss: 0.10580 | Test loss: 0.08026\n",
      "Epoch: 2550 | Train loss: 0.12061 | Test loss: 0.04795\n",
      "Epoch: 2560 | Train loss: 0.13258 | Test loss: 0.02205\n",
      "Epoch: 2570 | Train loss: 0.14317 | Test loss: 0.00240\n",
      "Epoch: 2580 | Train loss: 0.09509 | Test loss: 0.10509\n",
      "Epoch: 2590 | Train loss: 0.11220 | Test loss: 0.06733\n",
      "Epoch: 2600 | Train loss: 0.12623 | Test loss: 0.03663\n",
      "Epoch: 2610 | Train loss: 0.13682 | Test loss: 0.01232\n",
      "Epoch: 2620 | Train loss: 0.08180 | Test loss: 0.13398\n",
      "Epoch: 2630 | Train loss: 0.10224 | Test loss: 0.08853\n",
      "Epoch: 2640 | Train loss: 0.11781 | Test loss: 0.05441\n",
      "Epoch: 2650 | Train loss: 0.13047 | Test loss: 0.02690\n",
      "Epoch: 2660 | Train loss: 0.14106 | Test loss: 0.00260\n",
      "Epoch: 2670 | Train loss: 0.09066 | Test loss: 0.11336\n",
      "Epoch: 2680 | Train loss: 0.10940 | Test loss: 0.07379\n",
      "Epoch: 2690 | Train loss: 0.12342 | Test loss: 0.04148\n",
      "Epoch: 2700 | Train loss: 0.13471 | Test loss: 0.01717\n",
      "Epoch: 2710 | Train loss: 0.14530 | Test loss: 0.00712\n",
      "Epoch: 2720 | Train loss: 0.09867 | Test loss: 0.09681\n",
      "Epoch: 2730 | Train loss: 0.11501 | Test loss: 0.06086\n",
      "Epoch: 2740 | Train loss: 0.12836 | Test loss: 0.03175\n",
      "Epoch: 2750 | Train loss: 0.13895 | Test loss: 0.00746\n",
      "Epoch: 2760 | Train loss: 0.08624 | Test loss: 0.12366\n",
      "Epoch: 2770 | Train loss: 0.10582 | Test loss: 0.08024\n",
      "Epoch: 2780 | Train loss: 0.12063 | Test loss: 0.04793\n",
      "Epoch: 2790 | Train loss: 0.13260 | Test loss: 0.02203\n",
      "Epoch: 2800 | Train loss: 0.14319 | Test loss: 0.00241\n",
      "Epoch: 2810 | Train loss: 0.09510 | Test loss: 0.10507\n",
      "Epoch: 2820 | Train loss: 0.11222 | Test loss: 0.06731\n",
      "Epoch: 2830 | Train loss: 0.12625 | Test loss: 0.03660\n",
      "Epoch: 2840 | Train loss: 0.13684 | Test loss: 0.01230\n",
      "Epoch: 2850 | Train loss: 0.08182 | Test loss: 0.13396\n",
      "Epoch: 2860 | Train loss: 0.10226 | Test loss: 0.08851\n",
      "Epoch: 2870 | Train loss: 0.11783 | Test loss: 0.05438\n",
      "Epoch: 2880 | Train loss: 0.13049 | Test loss: 0.02688\n",
      "Epoch: 2890 | Train loss: 0.14108 | Test loss: 0.00258\n",
      "Epoch: 2900 | Train loss: 0.09067 | Test loss: 0.11334\n",
      "Epoch: 2910 | Train loss: 0.10942 | Test loss: 0.07377\n",
      "Epoch: 2920 | Train loss: 0.12344 | Test loss: 0.04145\n",
      "Epoch: 2930 | Train loss: 0.13473 | Test loss: 0.01715\n",
      "Epoch: 2940 | Train loss: 0.14532 | Test loss: 0.00715\n",
      "Epoch: 2950 | Train loss: 0.09869 | Test loss: 0.09678\n",
      "Epoch: 2960 | Train loss: 0.11503 | Test loss: 0.06083\n",
      "Epoch: 2970 | Train loss: 0.12838 | Test loss: 0.03173\n",
      "Epoch: 2980 | Train loss: 0.13897 | Test loss: 0.00743\n",
      "Epoch: 2990 | Train loss: 0.08626 | Test loss: 0.12364\n",
      "Epoch: 3000 | Train loss: 0.10584 | Test loss: 0.08022\n",
      "Epoch: 3010 | Train loss: 0.12065 | Test loss: 0.04791\n",
      "Epoch: 3020 | Train loss: 0.13262 | Test loss: 0.02200\n",
      "Epoch: 3030 | Train loss: 0.14321 | Test loss: 0.00243\n",
      "Epoch: 3040 | Train loss: 0.09512 | Test loss: 0.10505\n",
      "Epoch: 3050 | Train loss: 0.11224 | Test loss: 0.06729\n",
      "Epoch: 3060 | Train loss: 0.12627 | Test loss: 0.03658\n",
      "Epoch: 3070 | Train loss: 0.13686 | Test loss: 0.01228\n",
      "Epoch: 3080 | Train loss: 0.08184 | Test loss: 0.13394\n",
      "Epoch: 3090 | Train loss: 0.10227 | Test loss: 0.08849\n",
      "Epoch: 3100 | Train loss: 0.11785 | Test loss: 0.05436\n",
      "Epoch: 3110 | Train loss: 0.13051 | Test loss: 0.02686\n",
      "Epoch: 3120 | Train loss: 0.14110 | Test loss: 0.00255\n",
      "Epoch: 3130 | Train loss: 0.09069 | Test loss: 0.11332\n",
      "Epoch: 3140 | Train loss: 0.10944 | Test loss: 0.07374\n",
      "Epoch: 3150 | Train loss: 0.12346 | Test loss: 0.04143\n",
      "Epoch: 3160 | Train loss: 0.13475 | Test loss: 0.01713\n",
      "Epoch: 3170 | Train loss: 0.14534 | Test loss: 0.00717\n",
      "Epoch: 3180 | Train loss: 0.09871 | Test loss: 0.09676\n",
      "Epoch: 3190 | Train loss: 0.11505 | Test loss: 0.06081\n",
      "Epoch: 3200 | Train loss: 0.12840 | Test loss: 0.03170\n",
      "Epoch: 3210 | Train loss: 0.13899 | Test loss: 0.00740\n",
      "Epoch: 3220 | Train loss: 0.08628 | Test loss: 0.12361\n",
      "Epoch: 3230 | Train loss: 0.10586 | Test loss: 0.08019\n",
      "Epoch: 3240 | Train loss: 0.12066 | Test loss: 0.04789\n",
      "Epoch: 3250 | Train loss: 0.13264 | Test loss: 0.02198\n",
      "Epoch: 3260 | Train loss: 0.14323 | Test loss: 0.00245\n",
      "Epoch: 3270 | Train loss: 0.09514 | Test loss: 0.10502\n",
      "Epoch: 3280 | Train loss: 0.11225 | Test loss: 0.06727\n",
      "Epoch: 3290 | Train loss: 0.12629 | Test loss: 0.03656\n",
      "Epoch: 3300 | Train loss: 0.13688 | Test loss: 0.01226\n",
      "Epoch: 3310 | Train loss: 0.08186 | Test loss: 0.13391\n",
      "Epoch: 3320 | Train loss: 0.10229 | Test loss: 0.08846\n",
      "Epoch: 3330 | Train loss: 0.11787 | Test loss: 0.05434\n",
      "Epoch: 3340 | Train loss: 0.13053 | Test loss: 0.02683\n",
      "Epoch: 3350 | Train loss: 0.14112 | Test loss: 0.00253\n",
      "Epoch: 3360 | Train loss: 0.09071 | Test loss: 0.11329\n",
      "Epoch: 3370 | Train loss: 0.10946 | Test loss: 0.07371\n",
      "Epoch: 3380 | Train loss: 0.12348 | Test loss: 0.04141\n",
      "Epoch: 3390 | Train loss: 0.13477 | Test loss: 0.01711\n",
      "Epoch: 3400 | Train loss: 0.14536 | Test loss: 0.00720\n",
      "Epoch: 3410 | Train loss: 0.09872 | Test loss: 0.09673\n",
      "Epoch: 3420 | Train loss: 0.11507 | Test loss: 0.06079\n",
      "Epoch: 3430 | Train loss: 0.12842 | Test loss: 0.03168\n",
      "Epoch: 3440 | Train loss: 0.13901 | Test loss: 0.00737\n",
      "Epoch: 3450 | Train loss: 0.08629 | Test loss: 0.12359\n",
      "Epoch: 3460 | Train loss: 0.10588 | Test loss: 0.08017\n",
      "Epoch: 3470 | Train loss: 0.12068 | Test loss: 0.04786\n",
      "Epoch: 3480 | Train loss: 0.13266 | Test loss: 0.02196\n",
      "Epoch: 3490 | Train loss: 0.14325 | Test loss: 0.00246\n",
      "Epoch: 3500 | Train loss: 0.09516 | Test loss: 0.10500\n",
      "Epoch: 3510 | Train loss: 0.11227 | Test loss: 0.06724\n",
      "Epoch: 3520 | Train loss: 0.12630 | Test loss: 0.03653\n",
      "Epoch: 3530 | Train loss: 0.13690 | Test loss: 0.01223\n",
      "Epoch: 3540 | Train loss: 0.08187 | Test loss: 0.13389\n",
      "Epoch: 3550 | Train loss: 0.10231 | Test loss: 0.08844\n",
      "Epoch: 3560 | Train loss: 0.11789 | Test loss: 0.05431\n",
      "Epoch: 3570 | Train loss: 0.13055 | Test loss: 0.02681\n",
      "Epoch: 3580 | Train loss: 0.14114 | Test loss: 0.00251\n",
      "Epoch: 3590 | Train loss: 0.09073 | Test loss: 0.11327\n",
      "Epoch: 3600 | Train loss: 0.10948 | Test loss: 0.07369\n",
      "Epoch: 3610 | Train loss: 0.12350 | Test loss: 0.04138\n",
      "Epoch: 3620 | Train loss: 0.13479 | Test loss: 0.01708\n",
      "Epoch: 3630 | Train loss: 0.14538 | Test loss: 0.00722\n",
      "Epoch: 3640 | Train loss: 0.09874 | Test loss: 0.09671\n",
      "Epoch: 3650 | Train loss: 0.11509 | Test loss: 0.06077\n",
      "Epoch: 3660 | Train loss: 0.12844 | Test loss: 0.03166\n",
      "Epoch: 3670 | Train loss: 0.13903 | Test loss: 0.00737\n",
      "Epoch: 3680 | Train loss: 0.08631 | Test loss: 0.12357\n",
      "Epoch: 3690 | Train loss: 0.10589 | Test loss: 0.08015\n",
      "Epoch: 3700 | Train loss: 0.12070 | Test loss: 0.04784\n",
      "Epoch: 3710 | Train loss: 0.13268 | Test loss: 0.02194\n",
      "Epoch: 3720 | Train loss: 0.14327 | Test loss: 0.00248\n",
      "Epoch: 3730 | Train loss: 0.09518 | Test loss: 0.10498\n",
      "Epoch: 3740 | Train loss: 0.11229 | Test loss: 0.06722\n",
      "Epoch: 3750 | Train loss: 0.12632 | Test loss: 0.03651\n",
      "Epoch: 3760 | Train loss: 0.13692 | Test loss: 0.01221\n",
      "Epoch: 3770 | Train loss: 0.08189 | Test loss: 0.13387\n",
      "Epoch: 3780 | Train loss: 0.10233 | Test loss: 0.08842\n",
      "Epoch: 3790 | Train loss: 0.11790 | Test loss: 0.05429\n",
      "Epoch: 3800 | Train loss: 0.13057 | Test loss: 0.02678\n",
      "Epoch: 3810 | Train loss: 0.14116 | Test loss: 0.00249\n",
      "Epoch: 3820 | Train loss: 0.09074 | Test loss: 0.11325\n",
      "Epoch: 3830 | Train loss: 0.10950 | Test loss: 0.07367\n",
      "Epoch: 3840 | Train loss: 0.12352 | Test loss: 0.04136\n",
      "Epoch: 3850 | Train loss: 0.13481 | Test loss: 0.01706\n",
      "Epoch: 3860 | Train loss: 0.14540 | Test loss: 0.00724\n",
      "Epoch: 3870 | Train loss: 0.09876 | Test loss: 0.09669\n",
      "Epoch: 3880 | Train loss: 0.11511 | Test loss: 0.06074\n",
      "Epoch: 3890 | Train loss: 0.12845 | Test loss: 0.03164\n",
      "Epoch: 3900 | Train loss: 0.13905 | Test loss: 0.00734\n",
      "Epoch: 3910 | Train loss: 0.08632 | Test loss: 0.12355\n",
      "Epoch: 3920 | Train loss: 0.10591 | Test loss: 0.08013\n",
      "Epoch: 3930 | Train loss: 0.12072 | Test loss: 0.04781\n",
      "Epoch: 3940 | Train loss: 0.13270 | Test loss: 0.02191\n",
      "Epoch: 3950 | Train loss: 0.14329 | Test loss: 0.00249\n",
      "Epoch: 3960 | Train loss: 0.09519 | Test loss: 0.10496\n",
      "Epoch: 3970 | Train loss: 0.11231 | Test loss: 0.06720\n",
      "Epoch: 3980 | Train loss: 0.12634 | Test loss: 0.03649\n",
      "Epoch: 3990 | Train loss: 0.13694 | Test loss: 0.01219\n",
      "Epoch: 4000 | Train loss: 0.08191 | Test loss: 0.13384\n",
      "Epoch: 4010 | Train loss: 0.10235 | Test loss: 0.08839\n",
      "Epoch: 4020 | Train loss: 0.11792 | Test loss: 0.05427\n",
      "Epoch: 4030 | Train loss: 0.13058 | Test loss: 0.02676\n",
      "Epoch: 4040 | Train loss: 0.14118 | Test loss: 0.00247\n",
      "Epoch: 4050 | Train loss: 0.09076 | Test loss: 0.11322\n",
      "Epoch: 4060 | Train loss: 0.10951 | Test loss: 0.07365\n",
      "Epoch: 4070 | Train loss: 0.12354 | Test loss: 0.04134\n",
      "Epoch: 4080 | Train loss: 0.13483 | Test loss: 0.01703\n",
      "Epoch: 4090 | Train loss: 0.14542 | Test loss: 0.00727\n",
      "Epoch: 4100 | Train loss: 0.09878 | Test loss: 0.09666\n",
      "Epoch: 4110 | Train loss: 0.11473 | Test loss: 0.06163\n",
      "Epoch: 4120 | Train loss: 0.12808 | Test loss: 0.03252\n",
      "Epoch: 4130 | Train loss: 0.13867 | Test loss: 0.00822\n",
      "Epoch: 4140 | Train loss: 0.09889 | Test loss: 0.09642\n",
      "Epoch: 4150 | Train loss: 0.11445 | Test loss: 0.06229\n",
      "Epoch: 4160 | Train loss: 0.12779 | Test loss: 0.03318\n",
      "Epoch: 4170 | Train loss: 0.13838 | Test loss: 0.00888\n",
      "Epoch: 4180 | Train loss: 0.10567 | Test loss: 0.08071\n",
      "Epoch: 4190 | Train loss: 0.12047 | Test loss: 0.04840\n",
      "Epoch: 4200 | Train loss: 0.13245 | Test loss: 0.02250\n",
      "Epoch: 4210 | Train loss: 0.14304 | Test loss: 0.00209\n",
      "Epoch: 4220 | Train loss: 0.10007 | Test loss: 0.09369\n",
      "Epoch: 4230 | Train loss: 0.11563 | Test loss: 0.05956\n",
      "Epoch: 4240 | Train loss: 0.12829 | Test loss: 0.03205\n",
      "Epoch: 4250 | Train loss: 0.13888 | Test loss: 0.00775\n",
      "Epoch: 4260 | Train loss: 0.09299 | Test loss: 0.11008\n",
      "Epoch: 4270 | Train loss: 0.11010 | Test loss: 0.07232\n",
      "Epoch: 4280 | Train loss: 0.12412 | Test loss: 0.04161\n",
      "Epoch: 4290 | Train loss: 0.13471 | Test loss: 0.01731\n",
      "Epoch: 4300 | Train loss: 0.14531 | Test loss: 0.00699\n",
      "Epoch: 4310 | Train loss: 0.09867 | Test loss: 0.09693\n",
      "Epoch: 4320 | Train loss: 0.11502 | Test loss: 0.06099\n",
      "Epoch: 4330 | Train loss: 0.12836 | Test loss: 0.03189\n",
      "Epoch: 4340 | Train loss: 0.13896 | Test loss: 0.00759\n",
      "Epoch: 4350 | Train loss: 0.08623 | Test loss: 0.12380\n",
      "Epoch: 4360 | Train loss: 0.10582 | Test loss: 0.08037\n",
      "Epoch: 4370 | Train loss: 0.12063 | Test loss: 0.04806\n",
      "Epoch: 4380 | Train loss: 0.13260 | Test loss: 0.02216\n",
      "Epoch: 4390 | Train loss: 0.14320 | Test loss: 0.00231\n",
      "Epoch: 4400 | Train loss: 0.09510 | Test loss: 0.10521\n",
      "Epoch: 4410 | Train loss: 0.11143 | Test loss: 0.06926\n",
      "Epoch: 4420 | Train loss: 0.12546 | Test loss: 0.03855\n",
      "Epoch: 4430 | Train loss: 0.13605 | Test loss: 0.01425\n",
      "Epoch: 4440 | Train loss: 0.10907 | Test loss: 0.07471\n",
      "Epoch: 4450 | Train loss: 0.12309 | Test loss: 0.04239\n",
      "Epoch: 4460 | Train loss: 0.13438 | Test loss: 0.01809\n",
      "Epoch: 4470 | Train loss: 0.14497 | Test loss: 0.00621\n",
      "Epoch: 4480 | Train loss: 0.10878 | Test loss: 0.07537\n",
      "Epoch: 4490 | Train loss: 0.12280 | Test loss: 0.04306\n",
      "Epoch: 4500 | Train loss: 0.13409 | Test loss: 0.01876\n",
      "Epoch: 4510 | Train loss: 0.14469 | Test loss: 0.00554\n",
      "Epoch: 4520 | Train loss: 0.11481 | Test loss: 0.06148\n",
      "Epoch: 4530 | Train loss: 0.12816 | Test loss: 0.03237\n",
      "Epoch: 4540 | Train loss: 0.13875 | Test loss: 0.00807\n",
      "Epoch: 4550 | Train loss: 0.09287 | Test loss: 0.11040\n",
      "Epoch: 4560 | Train loss: 0.10997 | Test loss: 0.07264\n",
      "Epoch: 4570 | Train loss: 0.12399 | Test loss: 0.04193\n",
      "Epoch: 4580 | Train loss: 0.13459 | Test loss: 0.01763\n",
      "Epoch: 4590 | Train loss: 0.14518 | Test loss: 0.00667\n",
      "Epoch: 4600 | Train loss: 0.10366 | Test loss: 0.08540\n",
      "Epoch: 4610 | Train loss: 0.11846 | Test loss: 0.05309\n",
      "Epoch: 4620 | Train loss: 0.13042 | Test loss: 0.02718\n",
      "Epoch: 4630 | Train loss: 0.14102 | Test loss: 0.00288\n",
      "Epoch: 4640 | Train loss: 0.09658 | Test loss: 0.10179\n",
      "Epoch: 4650 | Train loss: 0.11292 | Test loss: 0.06585\n",
      "Epoch: 4660 | Train loss: 0.12626 | Test loss: 0.03674\n",
      "Epoch: 4670 | Train loss: 0.13685 | Test loss: 0.01244\n",
      "Epoch: 4680 | Train loss: 0.08182 | Test loss: 0.13410\n",
      "Epoch: 4690 | Train loss: 0.10226 | Test loss: 0.08865\n",
      "Epoch: 4700 | Train loss: 0.11705 | Test loss: 0.05634\n",
      "Epoch: 4710 | Train loss: 0.12971 | Test loss: 0.02883\n",
      "Epoch: 4720 | Train loss: 0.14030 | Test loss: 0.00452\n",
      "Epoch: 4730 | Train loss: 0.11469 | Test loss: 0.06178\n",
      "Epoch: 4740 | Train loss: 0.12804 | Test loss: 0.03267\n",
      "Epoch: 4750 | Train loss: 0.13863 | Test loss: 0.00837\n",
      "Epoch: 4760 | Train loss: 0.09884 | Test loss: 0.09657\n",
      "Epoch: 4770 | Train loss: 0.11440 | Test loss: 0.06244\n",
      "Epoch: 4780 | Train loss: 0.12775 | Test loss: 0.03334\n",
      "Epoch: 4790 | Train loss: 0.13834 | Test loss: 0.00904\n",
      "Epoch: 4800 | Train loss: 0.10562 | Test loss: 0.08086\n",
      "Epoch: 4810 | Train loss: 0.12043 | Test loss: 0.04856\n",
      "Epoch: 4820 | Train loss: 0.13241 | Test loss: 0.02265\n",
      "Epoch: 4830 | Train loss: 0.14300 | Test loss: 0.00200\n",
      "Epoch: 4840 | Train loss: 0.10002 | Test loss: 0.09384\n",
      "Epoch: 4850 | Train loss: 0.11559 | Test loss: 0.05971\n",
      "Epoch: 4860 | Train loss: 0.12824 | Test loss: 0.03221\n",
      "Epoch: 4870 | Train loss: 0.13884 | Test loss: 0.00791\n",
      "Epoch: 4880 | Train loss: 0.09295 | Test loss: 0.11023\n",
      "Epoch: 4890 | Train loss: 0.11006 | Test loss: 0.07247\n",
      "Epoch: 4900 | Train loss: 0.12408 | Test loss: 0.04176\n",
      "Epoch: 4910 | Train loss: 0.13467 | Test loss: 0.01746\n",
      "Epoch: 4920 | Train loss: 0.14527 | Test loss: 0.00684\n",
      "Epoch: 4930 | Train loss: 0.10374 | Test loss: 0.08523\n",
      "Epoch: 4940 | Train loss: 0.11854 | Test loss: 0.05292\n",
      "Epoch: 4950 | Train loss: 0.13051 | Test loss: 0.02702\n",
      "Epoch: 4960 | Train loss: 0.14110 | Test loss: 0.00272\n",
      "Epoch: 4970 | Train loss: 0.09068 | Test loss: 0.11348\n",
      "Epoch: 4980 | Train loss: 0.10865 | Test loss: 0.07572\n",
      "Epoch: 4990 | Train loss: 0.12267 | Test loss: 0.04341\n",
      "Epoch: 5000 | Train loss: 0.13396 | Test loss: 0.01911\n",
      "Epoch: 5010 | Train loss: 0.14455 | Test loss: 0.00519\n",
      "Epoch: 5020 | Train loss: 0.12031 | Test loss: 0.04885\n",
      "Epoch: 5030 | Train loss: 0.13228 | Test loss: 0.02295\n",
      "Epoch: 5040 | Train loss: 0.14288 | Test loss: 0.00184\n",
      "Epoch: 5050 | Train loss: 0.10600 | Test loss: 0.08183\n",
      "Epoch: 5060 | Train loss: 0.12002 | Test loss: 0.04951\n",
      "Epoch: 5070 | Train loss: 0.13200 | Test loss: 0.02361\n",
      "Epoch: 5080 | Train loss: 0.14259 | Test loss: 0.00159\n",
      "Epoch: 5090 | Train loss: 0.11124 | Test loss: 0.06975\n",
      "Epoch: 5100 | Train loss: 0.12527 | Test loss: 0.03904\n",
      "Epoch: 5110 | Train loss: 0.13587 | Test loss: 0.01475\n",
      "Epoch: 5120 | Train loss: 0.11618 | Test loss: 0.05838\n",
      "Epoch: 5130 | Train loss: 0.12884 | Test loss: 0.03087\n",
      "Epoch: 5140 | Train loss: 0.13943 | Test loss: 0.00657\n",
      "Epoch: 5150 | Train loss: 0.07538 | Test loss: 0.14695\n",
      "Epoch: 5160 | Train loss: 0.09760 | Test loss: 0.09947\n",
      "Epoch: 5170 | Train loss: 0.11395 | Test loss: 0.06352\n",
      "Epoch: 5180 | Train loss: 0.12729 | Test loss: 0.03442\n",
      "Epoch: 5190 | Train loss: 0.13788 | Test loss: 0.01012\n",
      "Epoch: 5200 | Train loss: 0.12559 | Test loss: 0.03832\n",
      "Epoch: 5210 | Train loss: 0.13619 | Test loss: 0.01401\n",
      "Epoch: 5220 | Train loss: 0.10920 | Test loss: 0.07447\n",
      "Epoch: 5230 | Train loss: 0.12322 | Test loss: 0.04216\n",
      "Epoch: 5240 | Train loss: 0.13451 | Test loss: 0.01786\n",
      "Epoch: 5250 | Train loss: 0.14511 | Test loss: 0.00644\n",
      "Epoch: 5260 | Train loss: 0.10358 | Test loss: 0.08563\n",
      "Epoch: 5270 | Train loss: 0.11838 | Test loss: 0.05332\n",
      "Epoch: 5280 | Train loss: 0.13035 | Test loss: 0.02741\n",
      "Epoch: 5290 | Train loss: 0.14094 | Test loss: 0.00311\n",
      "Epoch: 5300 | Train loss: 0.09651 | Test loss: 0.10201\n",
      "Epoch: 5310 | Train loss: 0.11285 | Test loss: 0.06607\n",
      "Epoch: 5320 | Train loss: 0.12619 | Test loss: 0.03697\n",
      "Epoch: 5330 | Train loss: 0.13678 | Test loss: 0.01268\n",
      "Epoch: 5340 | Train loss: 0.08858 | Test loss: 0.11841\n",
      "Epoch: 5350 | Train loss: 0.10731 | Test loss: 0.07883\n",
      "Epoch: 5360 | Train loss: 0.12133 | Test loss: 0.04652\n",
      "Epoch: 5370 | Train loss: 0.13262 | Test loss: 0.02222\n",
      "Epoch: 5380 | Train loss: 0.14321 | Test loss: 0.00226\n",
      "Epoch: 5390 | Train loss: 0.09511 | Test loss: 0.10527\n",
      "Epoch: 5400 | Train loss: 0.11144 | Test loss: 0.06932\n",
      "Epoch: 5410 | Train loss: 0.12547 | Test loss: 0.03861\n",
      "Epoch: 5420 | Train loss: 0.13606 | Test loss: 0.01431\n",
      "Epoch: 5430 | Train loss: 0.10908 | Test loss: 0.07477\n",
      "Epoch: 5440 | Train loss: 0.12310 | Test loss: 0.04246\n",
      "Epoch: 5450 | Train loss: 0.13439 | Test loss: 0.01815\n",
      "Epoch: 5460 | Train loss: 0.14498 | Test loss: 0.00614\n",
      "Epoch: 5470 | Train loss: 0.10879 | Test loss: 0.07543\n",
      "Epoch: 5480 | Train loss: 0.12281 | Test loss: 0.04312\n",
      "Epoch: 5490 | Train loss: 0.13410 | Test loss: 0.01882\n",
      "Epoch: 5500 | Train loss: 0.14470 | Test loss: 0.00548\n",
      "Epoch: 5510 | Train loss: 0.12045 | Test loss: 0.04856\n",
      "Epoch: 5520 | Train loss: 0.13173 | Test loss: 0.02426\n",
      "Epoch: 5530 | Train loss: 0.14232 | Test loss: 0.00147\n",
      "Epoch: 5540 | Train loss: 0.12470 | Test loss: 0.04039\n",
      "Epoch: 5550 | Train loss: 0.13530 | Test loss: 0.01609\n",
      "Epoch: 5560 | Train loss: 0.14589 | Test loss: 0.00821\n",
      "Epoch: 5570 | Train loss: 0.08969 | Test loss: 0.11582\n",
      "Epoch: 5580 | Train loss: 0.10844 | Test loss: 0.07625\n",
      "Epoch: 5590 | Train loss: 0.12246 | Test loss: 0.04394\n",
      "Epoch: 5600 | Train loss: 0.13375 | Test loss: 0.01964\n",
      "Epoch: 5610 | Train loss: 0.14434 | Test loss: 0.00466\n",
      "Epoch: 5620 | Train loss: 0.12974 | Test loss: 0.02885\n",
      "Epoch: 5630 | Train loss: 0.14033 | Test loss: 0.00454\n",
      "Epoch: 5640 | Train loss: 0.11471 | Test loss: 0.06179\n",
      "Epoch: 5650 | Train loss: 0.12736 | Test loss: 0.03429\n",
      "Epoch: 5660 | Train loss: 0.13796 | Test loss: 0.00999\n",
      "Epoch: 5670 | Train loss: 0.12567 | Test loss: 0.03819\n",
      "Epoch: 5680 | Train loss: 0.13626 | Test loss: 0.01389\n",
      "Epoch: 5690 | Train loss: 0.10208 | Test loss: 0.08913\n",
      "Epoch: 5700 | Train loss: 0.11687 | Test loss: 0.05682\n",
      "Epoch: 5710 | Train loss: 0.12953 | Test loss: 0.02932\n",
      "Epoch: 5720 | Train loss: 0.14013 | Test loss: 0.00502\n",
      "Epoch: 5730 | Train loss: 0.12181 | Test loss: 0.04545\n",
      "Epoch: 5740 | Train loss: 0.13310 | Test loss: 0.02115\n",
      "Epoch: 5750 | Train loss: 0.14369 | Test loss: 0.00316\n",
      "Epoch: 5760 | Train loss: 0.08519 | Test loss: 0.12633\n",
      "Epoch: 5770 | Train loss: 0.10477 | Test loss: 0.08291\n",
      "Epoch: 5780 | Train loss: 0.11958 | Test loss: 0.05059\n",
      "Epoch: 5790 | Train loss: 0.13155 | Test loss: 0.02469\n",
      "Epoch: 5800 | Train loss: 0.14215 | Test loss: 0.00147\n",
      "Epoch: 5810 | Train loss: 0.12986 | Test loss: 0.02859\n",
      "Epoch: 5820 | Train loss: 0.14045 | Test loss: 0.00429\n",
      "Epoch: 5830 | Train loss: 0.11483 | Test loss: 0.06154\n",
      "Epoch: 5840 | Train loss: 0.12748 | Test loss: 0.03403\n",
      "Epoch: 5850 | Train loss: 0.13808 | Test loss: 0.00973\n",
      "Epoch: 5860 | Train loss: 0.11908 | Test loss: 0.05176\n",
      "Epoch: 5870 | Train loss: 0.13105 | Test loss: 0.02586\n",
      "Epoch: 5880 | Train loss: 0.14164 | Test loss: 0.00182\n",
      "Epoch: 5890 | Train loss: 0.08083 | Test loss: 0.13649\n",
      "Epoch: 5900 | Train loss: 0.10127 | Test loss: 0.09104\n",
      "Epoch: 5910 | Train loss: 0.11684 | Test loss: 0.05691\n",
      "Epoch: 5920 | Train loss: 0.12950 | Test loss: 0.02941\n",
      "Epoch: 5930 | Train loss: 0.14010 | Test loss: 0.00510\n",
      "Epoch: 5940 | Train loss: 0.12781 | Test loss: 0.03330\n",
      "Epoch: 5950 | Train loss: 0.13840 | Test loss: 0.00900\n",
      "Epoch: 5960 | Train loss: 0.10568 | Test loss: 0.08265\n",
      "Epoch: 5970 | Train loss: 0.11970 | Test loss: 0.05034\n",
      "Epoch: 5980 | Train loss: 0.13167 | Test loss: 0.02443\n",
      "Epoch: 5990 | Train loss: 0.14227 | Test loss: 0.00146\n",
      "Epoch: 6000 | Train loss: 0.12464 | Test loss: 0.04056\n",
      "Epoch: 6010 | Train loss: 0.13524 | Test loss: 0.01626\n",
      "Epoch: 6020 | Train loss: 0.14583 | Test loss: 0.00804\n",
      "Epoch: 6030 | Train loss: 0.08963 | Test loss: 0.11599\n",
      "Epoch: 6040 | Train loss: 0.10838 | Test loss: 0.07642\n",
      "Epoch: 6050 | Train loss: 0.12241 | Test loss: 0.04411\n",
      "Epoch: 6060 | Train loss: 0.13369 | Test loss: 0.01981\n",
      "Epoch: 6070 | Train loss: 0.14429 | Test loss: 0.00449\n",
      "Epoch: 6080 | Train loss: 0.13200 | Test loss: 0.02370\n",
      "Epoch: 6090 | Train loss: 0.14259 | Test loss: 0.00156\n",
      "Epoch: 6100 | Train loss: 0.11766 | Test loss: 0.05505\n",
      "Epoch: 6110 | Train loss: 0.12962 | Test loss: 0.02915\n",
      "Epoch: 6120 | Train loss: 0.14022 | Test loss: 0.00485\n",
      "Epoch: 6130 | Train loss: 0.12190 | Test loss: 0.04528\n",
      "Epoch: 6140 | Train loss: 0.13319 | Test loss: 0.02097\n",
      "Epoch: 6150 | Train loss: 0.14378 | Test loss: 0.00333\n",
      "Epoch: 6160 | Train loss: 0.08528 | Test loss: 0.12616\n",
      "Epoch: 6170 | Train loss: 0.10486 | Test loss: 0.08273\n",
      "Epoch: 6180 | Train loss: 0.11967 | Test loss: 0.05042\n",
      "Epoch: 6190 | Train loss: 0.13164 | Test loss: 0.02452\n",
      "Epoch: 6200 | Train loss: 0.14224 | Test loss: 0.00146\n",
      "Epoch: 6210 | Train loss: 0.12995 | Test loss: 0.02842\n",
      "Epoch: 6220 | Train loss: 0.14054 | Test loss: 0.00412\n",
      "Epoch: 6230 | Train loss: 0.10850 | Test loss: 0.07616\n",
      "Epoch: 6240 | Train loss: 0.12252 | Test loss: 0.04385\n",
      "Epoch: 6250 | Train loss: 0.13381 | Test loss: 0.01955\n",
      "Epoch: 6260 | Train loss: 0.14441 | Test loss: 0.00475\n",
      "Epoch: 6270 | Train loss: 0.12678 | Test loss: 0.03567\n",
      "Epoch: 6280 | Train loss: 0.13738 | Test loss: 0.01137\n",
      "Epoch: 6290 | Train loss: 0.06904 | Test loss: 0.15947\n",
      "Epoch: 6300 | Train loss: 0.09409 | Test loss: 0.10769\n",
      "Epoch: 6310 | Train loss: 0.11121 | Test loss: 0.06993\n",
      "Epoch: 6320 | Train loss: 0.12524 | Test loss: 0.03922\n",
      "Epoch: 6330 | Train loss: 0.13583 | Test loss: 0.01492\n",
      "Epoch: 6340 | Train loss: 0.12354 | Test loss: 0.04312\n",
      "Epoch: 6350 | Train loss: 0.13414 | Test loss: 0.01882\n",
      "Epoch: 6360 | Train loss: 0.14473 | Test loss: 0.00548\n",
      "Epoch: 6370 | Train loss: 0.11688 | Test loss: 0.05687\n",
      "Epoch: 6380 | Train loss: 0.12954 | Test loss: 0.02936\n",
      "Epoch: 6390 | Train loss: 0.14013 | Test loss: 0.00506\n",
      "Epoch: 6400 | Train loss: 0.12553 | Test loss: 0.03857\n",
      "Epoch: 6410 | Train loss: 0.13612 | Test loss: 0.01427\n",
      "Epoch: 6420 | Train loss: 0.10913 | Test loss: 0.07473\n",
      "Epoch: 6430 | Train loss: 0.12315 | Test loss: 0.04402\n",
      "Epoch: 6440 | Train loss: 0.13375 | Test loss: 0.01972\n",
      "Epoch: 6450 | Train loss: 0.14434 | Test loss: 0.00458\n",
      "Epoch: 6460 | Train loss: 0.13205 | Test loss: 0.02361\n",
      "Epoch: 6470 | Train loss: 0.14264 | Test loss: 0.00158\n",
      "Epoch: 6480 | Train loss: 0.11129 | Test loss: 0.06975\n",
      "Epoch: 6490 | Train loss: 0.12532 | Test loss: 0.03904\n",
      "Epoch: 6500 | Train loss: 0.13592 | Test loss: 0.01474\n",
      "Epoch: 6510 | Train loss: 0.11623 | Test loss: 0.05837\n",
      "Epoch: 6520 | Train loss: 0.12889 | Test loss: 0.03087\n",
      "Epoch: 6530 | Train loss: 0.13948 | Test loss: 0.00657\n",
      "Epoch: 6540 | Train loss: 0.07542 | Test loss: 0.14695\n",
      "Epoch: 6550 | Train loss: 0.09765 | Test loss: 0.09947\n",
      "Epoch: 6560 | Train loss: 0.11400 | Test loss: 0.06352\n",
      "Epoch: 6570 | Train loss: 0.12734 | Test loss: 0.03441\n",
      "Epoch: 6580 | Train loss: 0.13794 | Test loss: 0.01011\n",
      "Epoch: 6590 | Train loss: 0.12565 | Test loss: 0.03831\n",
      "Epoch: 6600 | Train loss: 0.13624 | Test loss: 0.01401\n",
      "Epoch: 6610 | Train loss: 0.10925 | Test loss: 0.07447\n",
      "Epoch: 6620 | Train loss: 0.12327 | Test loss: 0.04376\n",
      "Epoch: 6630 | Train loss: 0.13387 | Test loss: 0.01945\n",
      "Epoch: 6640 | Train loss: 0.14446 | Test loss: 0.00484\n",
      "Epoch: 6650 | Train loss: 0.12684 | Test loss: 0.03559\n",
      "Epoch: 6660 | Train loss: 0.13743 | Test loss: 0.01128\n",
      "Epoch: 6670 | Train loss: 0.06909 | Test loss: 0.15938\n",
      "Epoch: 6680 | Train loss: 0.09414 | Test loss: 0.10760\n",
      "Epoch: 6690 | Train loss: 0.11126 | Test loss: 0.06984\n",
      "Epoch: 6700 | Train loss: 0.12529 | Test loss: 0.03913\n",
      "Epoch: 6710 | Train loss: 0.13589 | Test loss: 0.01483\n",
      "Epoch: 6720 | Train loss: 0.12360 | Test loss: 0.04303\n",
      "Epoch: 6730 | Train loss: 0.13419 | Test loss: 0.01873\n",
      "Epoch: 6740 | Train loss: 0.14478 | Test loss: 0.00557\n",
      "Epoch: 6750 | Train loss: 0.11412 | Test loss: 0.06327\n",
      "Epoch: 6760 | Train loss: 0.12746 | Test loss: 0.03416\n",
      "Epoch: 6770 | Train loss: 0.13806 | Test loss: 0.00986\n",
      "Epoch: 6780 | Train loss: 0.11905 | Test loss: 0.05189\n",
      "Epoch: 6790 | Train loss: 0.13103 | Test loss: 0.02599\n",
      "Epoch: 6800 | Train loss: 0.14162 | Test loss: 0.00189\n",
      "Epoch: 6810 | Train loss: 0.08081 | Test loss: 0.13661\n",
      "Epoch: 6820 | Train loss: 0.10124 | Test loss: 0.09116\n",
      "Epoch: 6830 | Train loss: 0.11682 | Test loss: 0.05704\n",
      "Epoch: 6840 | Train loss: 0.12948 | Test loss: 0.02953\n",
      "Epoch: 6850 | Train loss: 0.14008 | Test loss: 0.00523\n",
      "Epoch: 6860 | Train loss: 0.12779 | Test loss: 0.03343\n",
      "Epoch: 6870 | Train loss: 0.13838 | Test loss: 0.00913\n",
      "Epoch: 6880 | Train loss: 0.11208 | Test loss: 0.06798\n",
      "Epoch: 6890 | Train loss: 0.12541 | Test loss: 0.03887\n",
      "Epoch: 6900 | Train loss: 0.13601 | Test loss: 0.01457\n",
      "Epoch: 6910 | Train loss: 0.11632 | Test loss: 0.05820\n",
      "Epoch: 6920 | Train loss: 0.12898 | Test loss: 0.03070\n",
      "Epoch: 6930 | Train loss: 0.13957 | Test loss: 0.00640\n",
      "Epoch: 6940 | Train loss: 0.07551 | Test loss: 0.14677\n",
      "Epoch: 6950 | Train loss: 0.09774 | Test loss: 0.09930\n",
      "Epoch: 6960 | Train loss: 0.11330 | Test loss: 0.06517\n",
      "Epoch: 6970 | Train loss: 0.12664 | Test loss: 0.03606\n",
      "Epoch: 6980 | Train loss: 0.13724 | Test loss: 0.01176\n",
      "Epoch: 6990 | Train loss: 0.07548 | Test loss: 0.14685\n",
      "Epoch: 7000 | Train loss: 0.09771 | Test loss: 0.09937\n",
      "Epoch: 7010 | Train loss: 0.11327 | Test loss: 0.06524\n",
      "Epoch: 7020 | Train loss: 0.12661 | Test loss: 0.03614\n",
      "Epoch: 7030 | Train loss: 0.13721 | Test loss: 0.01183\n",
      "Epoch: 7040 | Train loss: 0.07545 | Test loss: 0.14692\n",
      "Epoch: 7050 | Train loss: 0.09768 | Test loss: 0.09945\n",
      "Epoch: 7060 | Train loss: 0.11324 | Test loss: 0.06532\n",
      "Epoch: 7070 | Train loss: 0.12658 | Test loss: 0.03621\n",
      "Epoch: 7080 | Train loss: 0.13717 | Test loss: 0.01191\n",
      "Epoch: 7090 | Train loss: 0.07542 | Test loss: 0.14700\n",
      "Epoch: 7100 | Train loss: 0.09765 | Test loss: 0.09952\n",
      "Epoch: 7110 | Train loss: 0.11321 | Test loss: 0.06539\n",
      "Epoch: 7120 | Train loss: 0.12655 | Test loss: 0.03628\n",
      "Epoch: 7130 | Train loss: 0.13714 | Test loss: 0.01199\n",
      "Epoch: 7140 | Train loss: 0.07539 | Test loss: 0.14707\n",
      "Epoch: 7150 | Train loss: 0.09762 | Test loss: 0.09960\n",
      "Epoch: 7160 | Train loss: 0.11317 | Test loss: 0.06547\n",
      "Epoch: 7170 | Train loss: 0.12652 | Test loss: 0.03636\n",
      "Epoch: 7180 | Train loss: 0.13711 | Test loss: 0.01206\n",
      "Epoch: 7190 | Train loss: 0.07536 | Test loss: 0.14715\n",
      "Epoch: 7200 | Train loss: 0.09758 | Test loss: 0.09967\n",
      "Epoch: 7210 | Train loss: 0.11314 | Test loss: 0.06554\n",
      "Epoch: 7220 | Train loss: 0.12649 | Test loss: 0.03644\n",
      "Epoch: 7230 | Train loss: 0.13708 | Test loss: 0.01214\n",
      "Epoch: 7240 | Train loss: 0.07533 | Test loss: 0.14722\n",
      "Epoch: 7250 | Train loss: 0.09755 | Test loss: 0.09974\n",
      "Epoch: 7260 | Train loss: 0.11311 | Test loss: 0.06562\n",
      "Epoch: 7270 | Train loss: 0.12646 | Test loss: 0.03651\n",
      "Epoch: 7280 | Train loss: 0.13705 | Test loss: 0.01221\n",
      "Epoch: 7290 | Train loss: 0.08200 | Test loss: 0.13387\n",
      "Epoch: 7300 | Train loss: 0.10245 | Test loss: 0.08842\n",
      "Epoch: 7310 | Train loss: 0.11724 | Test loss: 0.05611\n",
      "Epoch: 7320 | Train loss: 0.12921 | Test loss: 0.03020\n",
      "Epoch: 7330 | Train loss: 0.13980 | Test loss: 0.00590\n",
      "Epoch: 7340 | Train loss: 0.14267 | Test loss: 0.00157\n",
      "Epoch: 7350 | Train loss: 0.11131 | Test loss: 0.06977\n",
      "Epoch: 7360 | Train loss: 0.12465 | Test loss: 0.04067\n",
      "Epoch: 7370 | Train loss: 0.13524 | Test loss: 0.01637\n",
      "Epoch: 7380 | Train loss: 0.14584 | Test loss: 0.00793\n",
      "Epoch: 7390 | Train loss: 0.08963 | Test loss: 0.11610\n",
      "Epoch: 7400 | Train loss: 0.10760 | Test loss: 0.07834\n",
      "Epoch: 7410 | Train loss: 0.12162 | Test loss: 0.04603\n",
      "Epoch: 7420 | Train loss: 0.13291 | Test loss: 0.02173\n",
      "Epoch: 7430 | Train loss: 0.14350 | Test loss: 0.00264\n",
      "Epoch: 7440 | Train loss: 0.08960 | Test loss: 0.11618\n",
      "Epoch: 7450 | Train loss: 0.10757 | Test loss: 0.07842\n",
      "Epoch: 7460 | Train loss: 0.12159 | Test loss: 0.04610\n",
      "Epoch: 7470 | Train loss: 0.13288 | Test loss: 0.02180\n",
      "Epoch: 7480 | Train loss: 0.14347 | Test loss: 0.00257\n",
      "Epoch: 7490 | Train loss: 0.08957 | Test loss: 0.11625\n",
      "Epoch: 7500 | Train loss: 0.10754 | Test loss: 0.07849\n",
      "Epoch: 7510 | Train loss: 0.12156 | Test loss: 0.04618\n",
      "Epoch: 7520 | Train loss: 0.13285 | Test loss: 0.02188\n",
      "Epoch: 7530 | Train loss: 0.14344 | Test loss: 0.00251\n",
      "Epoch: 7540 | Train loss: 0.08954 | Test loss: 0.11632\n",
      "Epoch: 7550 | Train loss: 0.10751 | Test loss: 0.07857\n",
      "Epoch: 7560 | Train loss: 0.12153 | Test loss: 0.04625\n",
      "Epoch: 7570 | Train loss: 0.13282 | Test loss: 0.02196\n",
      "Epoch: 7580 | Train loss: 0.14341 | Test loss: 0.00245\n",
      "Epoch: 7590 | Train loss: 0.09486 | Test loss: 0.10601\n",
      "Epoch: 7600 | Train loss: 0.11119 | Test loss: 0.07007\n",
      "Epoch: 7610 | Train loss: 0.12453 | Test loss: 0.04096\n",
      "Epoch: 7620 | Train loss: 0.13512 | Test loss: 0.01666\n",
      "Epoch: 7630 | Train loss: 0.14572 | Test loss: 0.00764\n",
      "Epoch: 7640 | Train loss: 0.09414 | Test loss: 0.10769\n",
      "Epoch: 7650 | Train loss: 0.11047 | Test loss: 0.07175\n",
      "Epoch: 7660 | Train loss: 0.12450 | Test loss: 0.04104\n",
      "Epoch: 7670 | Train loss: 0.13509 | Test loss: 0.01674\n",
      "Epoch: 7680 | Train loss: 0.14569 | Test loss: 0.00756\n",
      "Epoch: 7690 | Train loss: 0.09411 | Test loss: 0.10776\n",
      "Epoch: 7700 | Train loss: 0.11044 | Test loss: 0.07182\n",
      "Epoch: 7710 | Train loss: 0.12447 | Test loss: 0.04111\n",
      "Epoch: 7720 | Train loss: 0.13506 | Test loss: 0.01681\n",
      "Epoch: 7730 | Train loss: 0.14566 | Test loss: 0.00749\n",
      "Epoch: 7740 | Train loss: 0.09408 | Test loss: 0.10784\n",
      "Epoch: 7750 | Train loss: 0.11041 | Test loss: 0.07190\n",
      "Epoch: 7760 | Train loss: 0.12444 | Test loss: 0.04119\n",
      "Epoch: 7770 | Train loss: 0.13503 | Test loss: 0.01689\n",
      "Epoch: 7780 | Train loss: 0.14563 | Test loss: 0.00741\n",
      "Epoch: 7790 | Train loss: 0.09405 | Test loss: 0.10791\n",
      "Epoch: 7800 | Train loss: 0.11038 | Test loss: 0.07197\n",
      "Epoch: 7810 | Train loss: 0.12441 | Test loss: 0.04126\n",
      "Epoch: 7820 | Train loss: 0.13500 | Test loss: 0.01696\n",
      "Epoch: 7830 | Train loss: 0.14559 | Test loss: 0.00734\n",
      "Epoch: 7840 | Train loss: 0.09402 | Test loss: 0.10799\n",
      "Epoch: 7850 | Train loss: 0.11035 | Test loss: 0.07205\n",
      "Epoch: 7860 | Train loss: 0.12438 | Test loss: 0.04134\n",
      "Epoch: 7870 | Train loss: 0.13497 | Test loss: 0.01704\n",
      "Epoch: 7880 | Train loss: 0.14556 | Test loss: 0.00727\n",
      "Epoch: 7890 | Train loss: 0.09650 | Test loss: 0.10224\n",
      "Epoch: 7900 | Train loss: 0.11284 | Test loss: 0.06630\n",
      "Epoch: 7910 | Train loss: 0.12619 | Test loss: 0.03719\n",
      "Epoch: 7920 | Train loss: 0.13678 | Test loss: 0.01289\n",
      "Epoch: 7930 | Train loss: 0.08856 | Test loss: 0.11864\n",
      "Epoch: 7940 | Train loss: 0.10731 | Test loss: 0.07906\n",
      "Epoch: 7950 | Train loss: 0.12133 | Test loss: 0.04675\n",
      "Epoch: 7960 | Train loss: 0.13262 | Test loss: 0.02245\n",
      "Epoch: 7970 | Train loss: 0.14321 | Test loss: 0.00211\n",
      "Epoch: 7980 | Train loss: 0.10022 | Test loss: 0.09364\n",
      "Epoch: 7990 | Train loss: 0.11579 | Test loss: 0.05951\n",
      "Epoch: 8000 | Train loss: 0.12845 | Test loss: 0.03200\n",
      "Epoch: 8010 | Train loss: 0.13904 | Test loss: 0.00770\n",
      "Epoch: 8020 | Train loss: 0.08630 | Test loss: 0.12391\n",
      "Epoch: 8030 | Train loss: 0.10502 | Test loss: 0.08434\n",
      "Epoch: 8040 | Train loss: 0.11904 | Test loss: 0.05203\n",
      "Epoch: 8050 | Train loss: 0.13102 | Test loss: 0.02612\n",
      "Epoch: 8060 | Train loss: 0.14161 | Test loss: 0.00197\n",
      "Epoch: 8070 | Train loss: 0.09657 | Test loss: 0.10209\n",
      "Epoch: 8080 | Train loss: 0.11291 | Test loss: 0.06615\n",
      "Epoch: 8090 | Train loss: 0.12626 | Test loss: 0.03705\n",
      "Epoch: 8100 | Train loss: 0.13685 | Test loss: 0.01274\n",
      "Epoch: 8110 | Train loss: 0.08863 | Test loss: 0.11848\n",
      "Epoch: 8120 | Train loss: 0.10738 | Test loss: 0.07891\n",
      "Epoch: 8130 | Train loss: 0.12140 | Test loss: 0.04660\n",
      "Epoch: 8140 | Train loss: 0.13269 | Test loss: 0.02230\n",
      "Epoch: 8150 | Train loss: 0.14328 | Test loss: 0.00220\n",
      "Epoch: 8160 | Train loss: 0.09429 | Test loss: 0.10737\n",
      "Epoch: 8170 | Train loss: 0.11063 | Test loss: 0.07143\n",
      "Epoch: 8180 | Train loss: 0.12466 | Test loss: 0.04072\n",
      "Epoch: 8190 | Train loss: 0.13525 | Test loss: 0.01642\n",
      "Epoch: 8200 | Train loss: 0.14585 | Test loss: 0.00652\n",
      "Epoch: 8210 | Train loss: 0.10372 | Test loss: 0.08555\n",
      "Epoch: 8220 | Train loss: 0.11852 | Test loss: 0.05324\n",
      "Epoch: 8230 | Train loss: 0.13050 | Test loss: 0.02733\n",
      "Epoch: 8240 | Train loss: 0.14109 | Test loss: 0.00303\n",
      "Epoch: 8250 | Train loss: 0.09664 | Test loss: 0.10194\n",
      "Epoch: 8260 | Train loss: 0.11299 | Test loss: 0.06600\n",
      "Epoch: 8270 | Train loss: 0.12633 | Test loss: 0.03689\n",
      "Epoch: 8280 | Train loss: 0.13693 | Test loss: 0.01259\n",
      "Epoch: 8290 | Train loss: 0.08187 | Test loss: 0.13425\n",
      "Epoch: 8300 | Train loss: 0.10144 | Test loss: 0.09083\n",
      "Epoch: 8310 | Train loss: 0.11623 | Test loss: 0.05852\n",
      "Epoch: 8320 | Train loss: 0.12890 | Test loss: 0.03101\n",
      "Epoch: 8330 | Train loss: 0.13949 | Test loss: 0.00671\n",
      "Epoch: 8340 | Train loss: 0.09299 | Test loss: 0.11040\n",
      "Epoch: 8350 | Train loss: 0.11011 | Test loss: 0.07264\n",
      "Epoch: 8360 | Train loss: 0.12414 | Test loss: 0.04193\n",
      "Epoch: 8370 | Train loss: 0.13473 | Test loss: 0.01763\n",
      "Epoch: 8380 | Train loss: 0.14533 | Test loss: 0.00668\n",
      "Epoch: 8390 | Train loss: 0.10379 | Test loss: 0.08540\n",
      "Epoch: 8400 | Train loss: 0.11859 | Test loss: 0.05309\n",
      "Epoch: 8410 | Train loss: 0.13057 | Test loss: 0.02718\n",
      "Epoch: 8420 | Train loss: 0.14116 | Test loss: 0.00288\n",
      "Epoch: 8430 | Train loss: 0.10853 | Test loss: 0.07629\n",
      "Epoch: 8440 | Train loss: 0.12255 | Test loss: 0.04558\n",
      "Epoch: 8450 | Train loss: 0.13314 | Test loss: 0.02128\n",
      "Epoch: 8460 | Train loss: 0.14374 | Test loss: 0.00303\n",
      "Epoch: 8470 | Train loss: 0.10015 | Test loss: 0.09383\n",
      "Epoch: 8480 | Train loss: 0.11573 | Test loss: 0.05970\n",
      "Epoch: 8490 | Train loss: 0.12839 | Test loss: 0.03220\n",
      "Epoch: 8500 | Train loss: 0.13898 | Test loss: 0.00790\n",
      "Epoch: 8510 | Train loss: 0.11208 | Test loss: 0.06811\n",
      "Epoch: 8520 | Train loss: 0.12542 | Test loss: 0.03900\n",
      "Epoch: 8530 | Train loss: 0.13601 | Test loss: 0.01470\n",
      "Epoch: 8540 | Train loss: 0.11632 | Test loss: 0.05833\n",
      "Epoch: 8550 | Train loss: 0.12829 | Test loss: 0.03243\n",
      "Epoch: 8560 | Train loss: 0.13888 | Test loss: 0.00813\n",
      "Epoch: 8570 | Train loss: 0.09298 | Test loss: 0.11045\n",
      "Epoch: 8580 | Train loss: 0.11009 | Test loss: 0.07269\n",
      "Epoch: 8590 | Train loss: 0.12412 | Test loss: 0.04199\n",
      "Epoch: 8600 | Train loss: 0.13472 | Test loss: 0.01769\n",
      "Epoch: 8610 | Train loss: 0.14531 | Test loss: 0.00525\n",
      "Epoch: 8620 | Train loss: 0.12046 | Test loss: 0.04879\n",
      "Epoch: 8630 | Train loss: 0.13175 | Test loss: 0.02449\n",
      "Epoch: 8640 | Train loss: 0.14234 | Test loss: 0.00145\n",
      "Epoch: 8650 | Train loss: 0.12402 | Test loss: 0.04222\n",
      "Epoch: 8660 | Train loss: 0.13462 | Test loss: 0.01793\n",
      "Epoch: 8670 | Train loss: 0.14521 | Test loss: 0.00571\n",
      "Epoch: 8680 | Train loss: 0.11425 | Test loss: 0.06313\n",
      "Epoch: 8690 | Train loss: 0.12690 | Test loss: 0.03562\n",
      "Epoch: 8700 | Train loss: 0.13749 | Test loss: 0.01132\n",
      "Epoch: 8710 | Train loss: 0.08867 | Test loss: 0.11843\n",
      "Epoch: 8720 | Train loss: 0.10742 | Test loss: 0.07885\n",
      "Epoch: 8730 | Train loss: 0.12145 | Test loss: 0.04654\n",
      "Epoch: 8740 | Train loss: 0.13274 | Test loss: 0.02224\n",
      "Epoch: 8750 | Train loss: 0.14333 | Test loss: 0.00224\n",
      "Epoch: 8760 | Train loss: 0.11138 | Test loss: 0.06974\n",
      "Epoch: 8770 | Train loss: 0.12472 | Test loss: 0.04063\n",
      "Epoch: 8780 | Train loss: 0.13531 | Test loss: 0.01633\n",
      "Epoch: 8790 | Train loss: 0.14591 | Test loss: 0.00661\n",
      "Epoch: 8800 | Train loss: 0.10377 | Test loss: 0.08546\n",
      "Epoch: 8810 | Train loss: 0.11858 | Test loss: 0.05315\n",
      "Epoch: 8820 | Train loss: 0.13055 | Test loss: 0.02725\n",
      "Epoch: 8830 | Train loss: 0.14115 | Test loss: 0.00295\n",
      "Epoch: 8840 | Train loss: 0.11493 | Test loss: 0.06157\n",
      "Epoch: 8850 | Train loss: 0.12759 | Test loss: 0.03406\n",
      "Epoch: 8860 | Train loss: 0.13818 | Test loss: 0.00976\n",
      "Epoch: 8870 | Train loss: 0.11917 | Test loss: 0.05179\n",
      "Epoch: 8880 | Train loss: 0.13045 | Test loss: 0.02748\n",
      "Epoch: 8890 | Train loss: 0.14105 | Test loss: 0.00319\n",
      "Epoch: 8900 | Train loss: 0.11483 | Test loss: 0.06180\n",
      "Epoch: 8910 | Train loss: 0.12749 | Test loss: 0.03429\n",
      "Epoch: 8920 | Train loss: 0.13808 | Test loss: 0.00999\n",
      "Epoch: 8930 | Train loss: 0.12579 | Test loss: 0.03819\n",
      "Epoch: 8940 | Train loss: 0.13638 | Test loss: 0.01389\n",
      "Epoch: 8950 | Train loss: 0.10219 | Test loss: 0.08914\n",
      "Epoch: 8960 | Train loss: 0.11699 | Test loss: 0.05682\n",
      "Epoch: 8970 | Train loss: 0.12896 | Test loss: 0.03092\n",
      "Epoch: 8980 | Train loss: 0.13955 | Test loss: 0.00662\n",
      "Epoch: 8990 | Train loss: 0.09305 | Test loss: 0.11031\n",
      "Epoch: 9000 | Train loss: 0.11017 | Test loss: 0.07255\n",
      "Epoch: 9010 | Train loss: 0.12420 | Test loss: 0.04184\n",
      "Epoch: 9020 | Train loss: 0.13480 | Test loss: 0.01754\n",
      "Epoch: 9030 | Train loss: 0.14539 | Test loss: 0.00540\n",
      "Epoch: 9040 | Train loss: 0.12054 | Test loss: 0.04865\n",
      "Epoch: 9050 | Train loss: 0.13183 | Test loss: 0.02434\n",
      "Epoch: 9060 | Train loss: 0.14242 | Test loss: 0.00145\n",
      "Epoch: 9070 | Train loss: 0.12410 | Test loss: 0.04208\n",
      "Epoch: 9080 | Train loss: 0.13469 | Test loss: 0.01778\n",
      "Epoch: 9090 | Train loss: 0.14529 | Test loss: 0.00517\n",
      "Epoch: 9100 | Train loss: 0.12044 | Test loss: 0.04888\n",
      "Epoch: 9110 | Train loss: 0.13173 | Test loss: 0.02458\n",
      "Epoch: 9120 | Train loss: 0.14232 | Test loss: 0.00145\n",
      "Epoch: 9130 | Train loss: 0.13003 | Test loss: 0.02848\n",
      "Epoch: 9140 | Train loss: 0.14063 | Test loss: 0.00418\n",
      "Epoch: 9150 | Train loss: 0.10858 | Test loss: 0.07622\n",
      "Epoch: 9160 | Train loss: 0.12261 | Test loss: 0.04551\n",
      "Epoch: 9170 | Train loss: 0.13320 | Test loss: 0.02121\n",
      "Epoch: 9180 | Train loss: 0.14379 | Test loss: 0.00309\n",
      "Epoch: 9190 | Train loss: 0.10020 | Test loss: 0.09376\n",
      "Epoch: 9200 | Train loss: 0.11578 | Test loss: 0.05963\n",
      "Epoch: 9210 | Train loss: 0.12844 | Test loss: 0.03213\n",
      "Epoch: 9220 | Train loss: 0.13904 | Test loss: 0.00783\n",
      "Epoch: 9230 | Train loss: 0.11213 | Test loss: 0.06804\n",
      "Epoch: 9240 | Train loss: 0.12548 | Test loss: 0.03893\n",
      "Epoch: 9250 | Train loss: 0.13607 | Test loss: 0.01463\n",
      "Epoch: 9260 | Train loss: 0.11638 | Test loss: 0.05827\n",
      "Epoch: 9270 | Train loss: 0.12834 | Test loss: 0.03236\n",
      "Epoch: 9280 | Train loss: 0.13894 | Test loss: 0.00806\n",
      "Epoch: 9290 | Train loss: 0.11203 | Test loss: 0.06827\n",
      "Epoch: 9300 | Train loss: 0.12538 | Test loss: 0.03917\n",
      "Epoch: 9310 | Train loss: 0.13597 | Test loss: 0.01487\n",
      "Epoch: 9320 | Train loss: 0.12368 | Test loss: 0.04307\n",
      "Epoch: 9330 | Train loss: 0.13427 | Test loss: 0.01877\n",
      "Epoch: 9340 | Train loss: 0.14487 | Test loss: 0.00417\n",
      "Epoch: 9350 | Train loss: 0.13951 | Test loss: 0.00675\n",
      "Epoch: 9360 | Train loss: 0.09301 | Test loss: 0.11043\n",
      "Epoch: 9370 | Train loss: 0.11013 | Test loss: 0.07268\n",
      "Epoch: 9380 | Train loss: 0.12416 | Test loss: 0.04196\n",
      "Epoch: 9390 | Train loss: 0.13475 | Test loss: 0.01766\n",
      "Epoch: 9400 | Train loss: 0.14535 | Test loss: 0.00527\n",
      "Epoch: 9410 | Train loss: 0.12050 | Test loss: 0.04877\n",
      "Epoch: 9420 | Train loss: 0.13179 | Test loss: 0.02447\n",
      "Epoch: 9430 | Train loss: 0.14238 | Test loss: 0.00144\n",
      "Epoch: 9440 | Train loss: 0.12406 | Test loss: 0.04220\n",
      "Epoch: 9450 | Train loss: 0.13465 | Test loss: 0.01790\n",
      "Epoch: 9460 | Train loss: 0.14525 | Test loss: 0.00504\n",
      "Epoch: 9470 | Train loss: 0.12040 | Test loss: 0.04901\n",
      "Epoch: 9480 | Train loss: 0.13169 | Test loss: 0.02471\n",
      "Epoch: 9490 | Train loss: 0.14228 | Test loss: 0.00145\n",
      "Epoch: 9500 | Train loss: 0.12999 | Test loss: 0.02861\n",
      "Epoch: 9510 | Train loss: 0.14058 | Test loss: 0.00430\n",
      "Epoch: 9520 | Train loss: 0.13523 | Test loss: 0.01658\n",
      "Epoch: 9530 | Train loss: 0.14583 | Test loss: 0.00635\n",
      "Epoch: 9540 | Train loss: 0.10369 | Test loss: 0.08572\n",
      "Epoch: 9550 | Train loss: 0.11850 | Test loss: 0.05340\n",
      "Epoch: 9560 | Train loss: 0.13047 | Test loss: 0.02750\n",
      "Epoch: 9570 | Train loss: 0.14107 | Test loss: 0.00320\n",
      "Epoch: 9580 | Train loss: 0.11485 | Test loss: 0.06181\n",
      "Epoch: 9590 | Train loss: 0.12751 | Test loss: 0.03431\n",
      "Epoch: 9600 | Train loss: 0.13810 | Test loss: 0.01001\n",
      "Epoch: 9610 | Train loss: 0.12581 | Test loss: 0.03821\n",
      "Epoch: 9620 | Train loss: 0.13640 | Test loss: 0.01390\n",
      "Epoch: 9630 | Train loss: 0.13105 | Test loss: 0.02619\n",
      "Epoch: 9640 | Train loss: 0.14164 | Test loss: 0.00201\n",
      "Epoch: 9650 | Train loss: 0.09659 | Test loss: 0.10215\n",
      "Epoch: 9660 | Train loss: 0.11294 | Test loss: 0.06621\n",
      "Epoch: 9670 | Train loss: 0.12559 | Test loss: 0.03871\n",
      "Epoch: 9680 | Train loss: 0.13619 | Test loss: 0.01441\n",
      "Epoch: 9690 | Train loss: 0.10919 | Test loss: 0.07486\n",
      "Epoch: 9700 | Train loss: 0.12322 | Test loss: 0.04415\n",
      "Epoch: 9710 | Train loss: 0.13381 | Test loss: 0.01985\n",
      "Epoch: 9720 | Train loss: 0.14441 | Test loss: 0.00445\n",
      "Epoch: 9730 | Train loss: 0.13211 | Test loss: 0.02375\n",
      "Epoch: 9740 | Train loss: 0.14271 | Test loss: 0.00153\n",
      "Epoch: 9750 | Train loss: 0.13735 | Test loss: 0.01173\n",
      "Epoch: 9760 | Train loss: 0.08853 | Test loss: 0.11884\n",
      "Epoch: 9770 | Train loss: 0.10728 | Test loss: 0.07926\n",
      "Epoch: 9780 | Train loss: 0.12131 | Test loss: 0.04855\n",
      "Epoch: 9790 | Train loss: 0.13190 | Test loss: 0.02425\n",
      "Epoch: 9800 | Train loss: 0.14249 | Test loss: 0.00145\n",
      "Epoch: 9810 | Train loss: 0.12991 | Test loss: 0.02881\n",
      "Epoch: 9820 | Train loss: 0.14050 | Test loss: 0.00451\n",
      "Epoch: 9830 | Train loss: 0.13515 | Test loss: 0.01679\n",
      "Epoch: 9840 | Train loss: 0.14575 | Test loss: 0.00614\n",
      "Epoch: 9850 | Train loss: 0.10895 | Test loss: 0.07543\n",
      "Epoch: 9860 | Train loss: 0.12298 | Test loss: 0.04472\n",
      "Epoch: 9870 | Train loss: 0.13357 | Test loss: 0.02042\n",
      "Epoch: 9880 | Train loss: 0.14416 | Test loss: 0.00388\n",
      "Epoch: 9890 | Train loss: 0.13940 | Test loss: 0.00704\n",
      "Epoch: 9900 | Train loss: 0.09900 | Test loss: 0.09660\n",
      "Epoch: 9910 | Train loss: 0.11457 | Test loss: 0.06248\n",
      "Epoch: 9920 | Train loss: 0.12723 | Test loss: 0.03497\n",
      "Epoch: 9930 | Train loss: 0.13782 | Test loss: 0.01067\n",
      "Epoch: 9940 | Train loss: 0.13306 | Test loss: 0.02159\n",
      "Epoch: 9950 | Train loss: 0.14366 | Test loss: 0.00274\n",
      "Epoch: 9960 | Train loss: 0.10617 | Test loss: 0.08183\n",
      "Epoch: 9970 | Train loss: 0.12020 | Test loss: 0.04952\n",
      "Epoch: 9980 | Train loss: 0.13148 | Test loss: 0.02522\n",
      "Epoch: 9990 | Train loss: 0.14208 | Test loss: 0.00154\n",
      "Epoch: 10000 | Train loss: 0.13732 | Test loss: 0.01184\n",
      "Epoch: 10010 | Train loss: 0.09546 | Test loss: 0.10482\n",
      "Epoch: 10020 | Train loss: 0.11180 | Test loss: 0.06887\n",
      "Epoch: 10030 | Train loss: 0.12514 | Test loss: 0.03977\n",
      "Epoch: 10040 | Train loss: 0.13574 | Test loss: 0.01547\n",
      "Epoch: 10050 | Train loss: 0.13098 | Test loss: 0.02639\n",
      "Epoch: 10060 | Train loss: 0.14157 | Test loss: 0.00215\n",
      "Epoch: 10070 | Train loss: 0.10262 | Test loss: 0.08823\n",
      "Epoch: 10080 | Train loss: 0.11742 | Test loss: 0.05592\n",
      "Epoch: 10090 | Train loss: 0.12940 | Test loss: 0.03001\n",
      "Epoch: 10100 | Train loss: 0.13999 | Test loss: 0.00571\n",
      "Epoch: 10110 | Train loss: 0.13523 | Test loss: 0.01664\n",
      "Epoch: 10120 | Train loss: 0.14583 | Test loss: 0.00630\n",
      "Epoch: 10130 | Train loss: 0.10368 | Test loss: 0.08758\n",
      "Epoch: 10140 | Train loss: 0.11771 | Test loss: 0.05527\n",
      "Epoch: 10150 | Train loss: 0.12968 | Test loss: 0.02937\n",
      "Epoch: 10160 | Train loss: 0.14027 | Test loss: 0.00507\n",
      "Epoch: 10170 | Train loss: 0.08633 | Test loss: 0.12400\n",
      "Epoch: 10180 | Train loss: 0.10506 | Test loss: 0.08443\n",
      "Epoch: 10190 | Train loss: 0.11908 | Test loss: 0.05212\n",
      "Epoch: 10200 | Train loss: 0.13036 | Test loss: 0.02781\n",
      "Epoch: 10210 | Train loss: 0.14095 | Test loss: 0.00351\n",
      "Epoch: 10220 | Train loss: 0.12204 | Test loss: 0.04690\n",
      "Epoch: 10230 | Train loss: 0.13263 | Test loss: 0.02261\n",
      "Epoch: 10240 | Train loss: 0.14322 | Test loss: 0.00200\n",
      "Epoch: 10250 | Train loss: 0.11769 | Test loss: 0.05532\n",
      "Epoch: 10260 | Train loss: 0.12967 | Test loss: 0.02941\n",
      "Epoch: 10270 | Train loss: 0.14026 | Test loss: 0.00511\n",
      "Epoch: 10280 | Train loss: 0.08631 | Test loss: 0.12405\n",
      "Epoch: 10290 | Train loss: 0.10504 | Test loss: 0.08447\n",
      "Epoch: 10300 | Train loss: 0.11906 | Test loss: 0.05216\n",
      "Epoch: 10310 | Train loss: 0.13034 | Test loss: 0.02786\n",
      "Epoch: 10320 | Train loss: 0.14094 | Test loss: 0.00356\n",
      "Epoch: 10330 | Train loss: 0.12202 | Test loss: 0.04695\n",
      "Epoch: 10340 | Train loss: 0.13262 | Test loss: 0.02265\n",
      "Epoch: 10350 | Train loss: 0.14321 | Test loss: 0.00198\n",
      "Epoch: 10360 | Train loss: 0.11768 | Test loss: 0.05536\n",
      "Epoch: 10370 | Train loss: 0.12965 | Test loss: 0.02945\n",
      "Epoch: 10380 | Train loss: 0.14024 | Test loss: 0.00516\n",
      "Epoch: 10390 | Train loss: 0.09314 | Test loss: 0.11020\n",
      "Epoch: 10400 | Train loss: 0.10948 | Test loss: 0.07426\n",
      "Epoch: 10410 | Train loss: 0.12351 | Test loss: 0.04355\n",
      "Epoch: 10420 | Train loss: 0.13410 | Test loss: 0.01925\n",
      "Epoch: 10430 | Train loss: 0.14470 | Test loss: 0.00369\n",
      "Epoch: 10440 | Train loss: 0.13102 | Test loss: 0.02633\n",
      "Epoch: 10450 | Train loss: 0.14161 | Test loss: 0.00210\n",
      "Epoch: 10460 | Train loss: 0.09656 | Test loss: 0.10230\n",
      "Epoch: 10470 | Train loss: 0.11212 | Test loss: 0.06816\n",
      "Epoch: 10480 | Train loss: 0.12547 | Test loss: 0.03906\n",
      "Epoch: 10490 | Train loss: 0.13606 | Test loss: 0.01476\n",
      "Epoch: 10500 | Train loss: 0.14607 | Test loss: 0.00682\n",
      "Epoch: 10510 | Train loss: 0.09792 | Test loss: 0.09914\n",
      "Epoch: 10520 | Train loss: 0.11349 | Test loss: 0.06501\n",
      "Epoch: 10530 | Train loss: 0.12615 | Test loss: 0.03750\n",
      "Epoch: 10540 | Train loss: 0.13674 | Test loss: 0.01320\n",
      "Epoch: 10550 | Train loss: 0.11645 | Test loss: 0.05820\n",
      "Epoch: 10560 | Train loss: 0.12842 | Test loss: 0.03229\n",
      "Epoch: 10570 | Train loss: 0.13901 | Test loss: 0.00799\n",
      "Epoch: 10580 | Train loss: 0.11211 | Test loss: 0.06821\n",
      "Epoch: 10590 | Train loss: 0.12545 | Test loss: 0.03910\n",
      "Epoch: 10600 | Train loss: 0.13605 | Test loss: 0.01480\n",
      "Epoch: 10610 | Train loss: 0.14605 | Test loss: 0.00678\n",
      "Epoch: 10620 | Train loss: 0.09791 | Test loss: 0.09918\n",
      "Epoch: 10630 | Train loss: 0.11348 | Test loss: 0.06505\n",
      "Epoch: 10640 | Train loss: 0.12613 | Test loss: 0.03755\n",
      "Epoch: 10650 | Train loss: 0.13673 | Test loss: 0.01325\n",
      "Epoch: 10660 | Train loss: 0.11644 | Test loss: 0.05824\n",
      "Epoch: 10670 | Train loss: 0.12841 | Test loss: 0.03234\n",
      "Epoch: 10680 | Train loss: 0.13900 | Test loss: 0.00804\n",
      "Epoch: 10690 | Train loss: 0.11209 | Test loss: 0.06825\n",
      "Epoch: 10700 | Train loss: 0.12544 | Test loss: 0.03914\n",
      "Epoch: 10710 | Train loss: 0.13603 | Test loss: 0.01484\n",
      "Epoch: 10720 | Train loss: 0.14604 | Test loss: 0.00673\n",
      "Epoch: 10730 | Train loss: 0.09790 | Test loss: 0.09922\n",
      "Epoch: 10740 | Train loss: 0.11346 | Test loss: 0.06509\n",
      "Epoch: 10750 | Train loss: 0.12612 | Test loss: 0.03759\n",
      "Epoch: 10760 | Train loss: 0.13671 | Test loss: 0.01329\n",
      "Epoch: 10770 | Train loss: 0.11642 | Test loss: 0.05828\n",
      "Epoch: 10780 | Train loss: 0.12839 | Test loss: 0.03238\n",
      "Epoch: 10790 | Train loss: 0.13898 | Test loss: 0.00808\n",
      "Epoch: 10800 | Train loss: 0.11208 | Test loss: 0.06829\n",
      "Epoch: 10810 | Train loss: 0.12542 | Test loss: 0.03919\n",
      "Epoch: 10820 | Train loss: 0.13602 | Test loss: 0.01489\n",
      "Epoch: 10830 | Train loss: 0.14603 | Test loss: 0.00669\n",
      "Epoch: 10840 | Train loss: 0.10388 | Test loss: 0.08720\n",
      "Epoch: 10850 | Train loss: 0.11790 | Test loss: 0.05488\n",
      "Epoch: 10860 | Train loss: 0.12988 | Test loss: 0.02898\n",
      "Epoch: 10870 | Train loss: 0.14047 | Test loss: 0.00468\n",
      "Epoch: 10880 | Train loss: 0.14274 | Test loss: 0.00152\n",
      "Epoch: 10890 | Train loss: 0.13739 | Test loss: 0.01175\n",
      "Epoch: 10900 | Train loss: 0.08856 | Test loss: 0.11886\n",
      "Epoch: 10910 | Train loss: 0.10652 | Test loss: 0.08110\n",
      "Epoch: 10920 | Train loss: 0.12054 | Test loss: 0.04879\n",
      "Epoch: 10930 | Train loss: 0.13184 | Test loss: 0.02449\n",
      "Epoch: 10940 | Train loss: 0.14243 | Test loss: 0.00143\n",
      "Epoch: 10950 | Train loss: 0.09079 | Test loss: 0.11570\n",
      "Epoch: 10960 | Train loss: 0.10790 | Test loss: 0.07794\n",
      "Epoch: 10970 | Train loss: 0.12192 | Test loss: 0.04724\n",
      "Epoch: 10980 | Train loss: 0.13252 | Test loss: 0.02294\n",
      "Epoch: 10990 | Train loss: 0.14311 | Test loss: 0.00182\n",
      "Epoch: 11000 | Train loss: 0.12419 | Test loss: 0.04202\n",
      "Epoch: 11010 | Train loss: 0.13479 | Test loss: 0.01772\n",
      "Epoch: 11020 | Train loss: 0.14539 | Test loss: 0.00522\n",
      "Epoch: 11030 | Train loss: 0.12053 | Test loss: 0.04883\n",
      "Epoch: 11040 | Train loss: 0.13182 | Test loss: 0.02453\n",
      "Epoch: 11050 | Train loss: 0.14242 | Test loss: 0.00144\n",
      "Epoch: 11060 | Train loss: 0.09077 | Test loss: 0.11575\n",
      "Epoch: 11070 | Train loss: 0.10788 | Test loss: 0.07799\n",
      "Epoch: 11080 | Train loss: 0.12191 | Test loss: 0.04728\n",
      "Epoch: 11090 | Train loss: 0.13250 | Test loss: 0.02297\n",
      "Epoch: 11100 | Train loss: 0.14309 | Test loss: 0.00180\n",
      "Epoch: 11110 | Train loss: 0.12418 | Test loss: 0.04207\n",
      "Epoch: 11120 | Train loss: 0.13477 | Test loss: 0.01777\n",
      "Epoch: 11130 | Train loss: 0.14537 | Test loss: 0.00517\n",
      "Epoch: 11140 | Train loss: 0.12052 | Test loss: 0.04888\n",
      "Epoch: 11150 | Train loss: 0.13181 | Test loss: 0.02457\n",
      "Epoch: 11160 | Train loss: 0.14240 | Test loss: 0.00144\n",
      "Epoch: 11170 | Train loss: 0.09076 | Test loss: 0.11579\n",
      "Epoch: 11180 | Train loss: 0.10787 | Test loss: 0.07803\n",
      "Epoch: 11190 | Train loss: 0.12189 | Test loss: 0.04732\n",
      "Epoch: 11200 | Train loss: 0.13249 | Test loss: 0.02303\n",
      "Epoch: 11210 | Train loss: 0.14308 | Test loss: 0.00178\n",
      "Epoch: 11220 | Train loss: 0.12417 | Test loss: 0.04211\n",
      "Epoch: 11230 | Train loss: 0.13476 | Test loss: 0.01781\n",
      "Epoch: 11240 | Train loss: 0.14536 | Test loss: 0.00513\n",
      "Epoch: 11250 | Train loss: 0.12050 | Test loss: 0.04892\n",
      "Epoch: 11260 | Train loss: 0.13179 | Test loss: 0.02462\n",
      "Epoch: 11270 | Train loss: 0.14239 | Test loss: 0.00144\n",
      "Epoch: 11280 | Train loss: 0.09674 | Test loss: 0.10195\n",
      "Epoch: 11290 | Train loss: 0.11230 | Test loss: 0.06782\n",
      "Epoch: 11300 | Train loss: 0.12495 | Test loss: 0.04032\n",
      "Epoch: 11310 | Train loss: 0.13554 | Test loss: 0.01601\n",
      "Epoch: 11320 | Train loss: 0.14555 | Test loss: 0.00556\n",
      "Epoch: 11330 | Train loss: 0.11427 | Test loss: 0.06328\n",
      "Epoch: 11340 | Train loss: 0.12693 | Test loss: 0.03577\n",
      "Epoch: 11350 | Train loss: 0.13753 | Test loss: 0.01147\n",
      "Epoch: 11360 | Train loss: 0.10934 | Test loss: 0.07465\n",
      "Epoch: 11370 | Train loss: 0.12268 | Test loss: 0.04554\n",
      "Epoch: 11380 | Train loss: 0.13327 | Test loss: 0.02124\n",
      "Epoch: 11390 | Train loss: 0.14386 | Test loss: 0.00200\n",
      "Epoch: 11400 | Train loss: 0.11773 | Test loss: 0.05531\n",
      "Epoch: 11410 | Train loss: 0.12901 | Test loss: 0.03101\n",
      "Epoch: 11420 | Train loss: 0.13960 | Test loss: 0.00671\n",
      "Epoch: 11430 | Train loss: 0.11210 | Test loss: 0.06829\n",
      "Epoch: 11440 | Train loss: 0.12476 | Test loss: 0.04079\n",
      "Epoch: 11450 | Train loss: 0.13535 | Test loss: 0.01648\n",
      "Epoch: 11460 | Train loss: 0.14535 | Test loss: 0.00509\n",
      "Epoch: 11470 | Train loss: 0.12050 | Test loss: 0.05055\n",
      "Epoch: 11480 | Train loss: 0.13109 | Test loss: 0.02625\n",
      "Epoch: 11490 | Train loss: 0.14168 | Test loss: 0.00204\n",
      "Epoch: 11500 | Train loss: 0.11487 | Test loss: 0.06193\n",
      "Epoch: 11510 | Train loss: 0.12718 | Test loss: 0.03522\n",
      "Epoch: 11520 | Train loss: 0.13777 | Test loss: 0.01092\n",
      "Epoch: 11530 | Train loss: 0.10238 | Test loss: 0.08889\n",
      "Epoch: 11540 | Train loss: 0.11719 | Test loss: 0.05658\n",
      "Epoch: 11550 | Train loss: 0.12916 | Test loss: 0.03068\n",
      "Epoch: 11560 | Train loss: 0.13975 | Test loss: 0.00638\n",
      "Epoch: 11570 | Train loss: 0.10583 | Test loss: 0.08275\n",
      "Epoch: 11580 | Train loss: 0.11985 | Test loss: 0.05043\n",
      "Epoch: 11590 | Train loss: 0.13114 | Test loss: 0.02613\n",
      "Epoch: 11600 | Train loss: 0.14174 | Test loss: 0.00196\n",
      "Epoch: 11610 | Train loss: 0.11492 | Test loss: 0.06181\n",
      "Epoch: 11620 | Train loss: 0.12689 | Test loss: 0.03591\n",
      "Epoch: 11630 | Train loss: 0.13748 | Test loss: 0.01160\n",
      "Epoch: 11640 | Train loss: 0.10929 | Test loss: 0.07478\n",
      "Epoch: 11650 | Train loss: 0.12263 | Test loss: 0.04567\n",
      "Epoch: 11660 | Train loss: 0.13322 | Test loss: 0.02137\n",
      "Epoch: 11670 | Train loss: 0.14381 | Test loss: 0.00192\n",
      "Epoch: 11680 | Train loss: 0.11768 | Test loss: 0.05545\n",
      "Epoch: 11690 | Train loss: 0.12896 | Test loss: 0.03115\n",
      "Epoch: 11700 | Train loss: 0.13956 | Test loss: 0.00684\n",
      "Epoch: 11710 | Train loss: 0.11205 | Test loss: 0.06842\n",
      "Epoch: 11720 | Train loss: 0.12471 | Test loss: 0.04091\n",
      "Epoch: 11730 | Train loss: 0.13530 | Test loss: 0.01661\n",
      "Epoch: 11740 | Train loss: 0.14530 | Test loss: 0.00496\n",
      "Epoch: 11750 | Train loss: 0.12342 | Test loss: 0.04388\n",
      "Epoch: 11760 | Train loss: 0.13401 | Test loss: 0.01958\n",
      "Epoch: 11770 | Train loss: 0.14461 | Test loss: 0.00336\n",
      "Epoch: 11780 | Train loss: 0.09441 | Test loss: 0.10738\n",
      "Epoch: 11790 | Train loss: 0.11075 | Test loss: 0.07143\n",
      "Epoch: 11800 | Train loss: 0.12409 | Test loss: 0.04232\n",
      "Epoch: 11810 | Train loss: 0.13469 | Test loss: 0.01802\n",
      "Epoch: 11820 | Train loss: 0.14529 | Test loss: 0.00492\n",
      "Epoch: 11830 | Train loss: 0.12637 | Test loss: 0.03711\n",
      "Epoch: 11840 | Train loss: 0.13696 | Test loss: 0.01281\n",
      "Epoch: 11850 | Train loss: 0.10937 | Test loss: 0.07463\n",
      "Epoch: 11860 | Train loss: 0.12270 | Test loss: 0.04552\n",
      "Epoch: 11870 | Train loss: 0.13330 | Test loss: 0.02122\n",
      "Epoch: 11880 | Train loss: 0.14389 | Test loss: 0.00201\n",
      "Epoch: 11890 | Train loss: 0.11776 | Test loss: 0.05529\n",
      "Epoch: 11900 | Train loss: 0.12904 | Test loss: 0.03099\n",
      "Epoch: 11910 | Train loss: 0.13963 | Test loss: 0.00669\n",
      "Epoch: 11920 | Train loss: 0.11213 | Test loss: 0.06827\n",
      "Epoch: 11930 | Train loss: 0.12478 | Test loss: 0.04076\n",
      "Epoch: 11940 | Train loss: 0.13537 | Test loss: 0.01646\n",
      "Epoch: 11950 | Train loss: 0.14538 | Test loss: 0.00512\n",
      "Epoch: 11960 | Train loss: 0.12052 | Test loss: 0.05053\n",
      "Epoch: 11970 | Train loss: 0.13111 | Test loss: 0.02623\n",
      "Epoch: 11980 | Train loss: 0.14171 | Test loss: 0.00203\n",
      "Epoch: 11990 | Train loss: 0.11489 | Test loss: 0.06191\n",
      "Epoch: 12000 | Train loss: 0.12686 | Test loss: 0.03600\n",
      "Epoch: 12010 | Train loss: 0.13745 | Test loss: 0.01170\n",
      "Epoch: 12020 | Train loss: 0.10926 | Test loss: 0.07489\n",
      "Epoch: 12030 | Train loss: 0.12260 | Test loss: 0.04577\n",
      "Epoch: 12040 | Train loss: 0.13319 | Test loss: 0.02147\n",
      "Epoch: 12050 | Train loss: 0.14379 | Test loss: 0.00187\n",
      "Epoch: 12060 | Train loss: 0.12131 | Test loss: 0.04874\n",
      "Epoch: 12070 | Train loss: 0.13190 | Test loss: 0.02443\n",
      "Epoch: 12080 | Train loss: 0.14250 | Test loss: 0.00142\n",
      "Epoch: 12090 | Train loss: 0.09085 | Test loss: 0.11565\n",
      "Epoch: 12100 | Train loss: 0.10796 | Test loss: 0.07789\n",
      "Epoch: 12110 | Train loss: 0.12199 | Test loss: 0.04718\n",
      "Epoch: 12120 | Train loss: 0.13258 | Test loss: 0.02288\n",
      "Epoch: 12130 | Train loss: 0.14317 | Test loss: 0.00185\n",
      "Epoch: 12140 | Train loss: 0.12426 | Test loss: 0.04197\n",
      "Epoch: 12150 | Train loss: 0.13485 | Test loss: 0.01767\n",
      "Epoch: 12160 | Train loss: 0.14545 | Test loss: 0.00527\n",
      "Epoch: 12170 | Train loss: 0.12060 | Test loss: 0.05038\n",
      "Epoch: 12180 | Train loss: 0.13119 | Test loss: 0.02608\n",
      "Epoch: 12190 | Train loss: 0.14178 | Test loss: 0.00192\n",
      "Epoch: 12200 | Train loss: 0.11496 | Test loss: 0.06175\n",
      "Epoch: 12210 | Train loss: 0.12693 | Test loss: 0.03585\n",
      "Epoch: 12220 | Train loss: 0.13752 | Test loss: 0.01155\n",
      "Epoch: 12230 | Train loss: 0.10934 | Test loss: 0.07473\n",
      "Epoch: 12240 | Train loss: 0.12267 | Test loss: 0.04562\n",
      "Epoch: 12250 | Train loss: 0.13327 | Test loss: 0.02132\n",
      "Epoch: 12260 | Train loss: 0.14386 | Test loss: 0.00195\n",
      "Epoch: 12270 | Train loss: 0.11773 | Test loss: 0.05539\n",
      "Epoch: 12280 | Train loss: 0.12901 | Test loss: 0.03110\n",
      "Epoch: 12290 | Train loss: 0.13960 | Test loss: 0.00679\n",
      "Epoch: 12300 | Train loss: 0.11210 | Test loss: 0.06837\n",
      "Epoch: 12310 | Train loss: 0.12475 | Test loss: 0.04086\n",
      "Epoch: 12320 | Train loss: 0.13534 | Test loss: 0.01656\n",
      "Epoch: 12330 | Train loss: 0.14535 | Test loss: 0.00502\n",
      "Epoch: 12340 | Train loss: 0.12049 | Test loss: 0.05063\n",
      "Epoch: 12350 | Train loss: 0.13109 | Test loss: 0.02633\n",
      "Epoch: 12360 | Train loss: 0.14168 | Test loss: 0.00210\n",
      "Epoch: 12370 | Train loss: 0.11851 | Test loss: 0.05359\n",
      "Epoch: 12380 | Train loss: 0.12980 | Test loss: 0.02929\n",
      "Epoch: 12390 | Train loss: 0.14039 | Test loss: 0.00499\n",
      "Epoch: 12400 | Train loss: 0.08643 | Test loss: 0.12393\n",
      "Epoch: 12410 | Train loss: 0.10517 | Test loss: 0.08435\n",
      "Epoch: 12420 | Train loss: 0.11919 | Test loss: 0.05204\n",
      "Epoch: 12430 | Train loss: 0.13047 | Test loss: 0.02774\n",
      "Epoch: 12440 | Train loss: 0.14107 | Test loss: 0.00344\n",
      "Epoch: 12450 | Train loss: 0.12215 | Test loss: 0.04683\n",
      "Epoch: 12460 | Train loss: 0.13275 | Test loss: 0.02253\n",
      "Epoch: 12470 | Train loss: 0.14334 | Test loss: 0.00148\n",
      "Epoch: 12480 | Train loss: 0.13739 | Test loss: 0.01187\n",
      "Epoch: 12490 | Train loss: 0.11650 | Test loss: 0.05823\n",
      "Epoch: 12500 | Train loss: 0.12848 | Test loss: 0.03232\n",
      "Epoch: 12510 | Train loss: 0.13907 | Test loss: 0.00803\n",
      "Epoch: 12520 | Train loss: 0.13312 | Test loss: 0.02167\n",
      "Epoch: 12530 | Train loss: 0.14372 | Test loss: 0.00177\n",
      "Epoch: 12540 | Train loss: 0.12421 | Test loss: 0.04211\n",
      "Epoch: 12550 | Train loss: 0.13480 | Test loss: 0.01782\n",
      "Epoch: 12560 | Train loss: 0.14481 | Test loss: 0.00376\n",
      "Epoch: 12570 | Train loss: 0.13113 | Test loss: 0.02626\n",
      "Epoch: 12580 | Train loss: 0.14172 | Test loss: 0.00204\n",
      "Epoch: 12590 | Train loss: 0.11490 | Test loss: 0.06193\n",
      "Epoch: 12600 | Train loss: 0.12687 | Test loss: 0.03603\n",
      "Epoch: 12610 | Train loss: 0.13746 | Test loss: 0.01173\n",
      "Epoch: 12620 | Train loss: 0.10927 | Test loss: 0.07491\n",
      "Epoch: 12630 | Train loss: 0.12261 | Test loss: 0.04580\n",
      "Epoch: 12640 | Train loss: 0.13320 | Test loss: 0.02150\n",
      "Epoch: 12650 | Train loss: 0.14380 | Test loss: 0.00185\n",
      "Epoch: 12660 | Train loss: 0.12429 | Test loss: 0.04195\n",
      "Epoch: 12670 | Train loss: 0.13488 | Test loss: 0.01765\n",
      "Epoch: 12680 | Train loss: 0.14489 | Test loss: 0.00393\n",
      "Epoch: 12690 | Train loss: 0.13953 | Test loss: 0.00699\n",
      "Epoch: 12700 | Train loss: 0.11932 | Test loss: 0.05175\n",
      "Epoch: 12710 | Train loss: 0.13061 | Test loss: 0.02745\n",
      "Epoch: 12720 | Train loss: 0.14121 | Test loss: 0.00315\n",
      "Epoch: 12730 | Train loss: 0.13526 | Test loss: 0.01679\n",
      "Epoch: 12740 | Train loss: 0.14527 | Test loss: 0.00479\n",
      "Epoch: 12750 | Train loss: 0.12634 | Test loss: 0.03724\n",
      "Epoch: 12760 | Train loss: 0.13694 | Test loss: 0.01294\n",
      "Epoch: 12770 | Train loss: 0.13479 | Test loss: 0.01785\n",
      "Epoch: 12780 | Train loss: 0.14480 | Test loss: 0.00372\n",
      "Epoch: 12790 | Train loss: 0.13112 | Test loss: 0.02629\n",
      "Epoch: 12800 | Train loss: 0.14171 | Test loss: 0.00207\n",
      "Epoch: 12810 | Train loss: 0.11489 | Test loss: 0.06197\n",
      "Epoch: 12820 | Train loss: 0.12686 | Test loss: 0.03606\n",
      "Epoch: 12830 | Train loss: 0.13745 | Test loss: 0.01176\n",
      "Epoch: 12840 | Train loss: 0.11291 | Test loss: 0.06653\n",
      "Epoch: 12850 | Train loss: 0.12557 | Test loss: 0.03902\n",
      "Epoch: 12860 | Train loss: 0.13616 | Test loss: 0.01472\n",
      "Epoch: 12870 | Train loss: 0.14618 | Test loss: 0.00686\n",
      "Epoch: 12880 | Train loss: 0.09802 | Test loss: 0.09910\n",
      "Epoch: 12890 | Train loss: 0.11359 | Test loss: 0.06497\n",
      "Epoch: 12900 | Train loss: 0.12625 | Test loss: 0.03747\n",
      "Epoch: 12910 | Train loss: 0.13684 | Test loss: 0.01317\n",
      "Epoch: 12920 | Train loss: 0.13852 | Test loss: 0.00932\n",
      "Epoch: 12930 | Train loss: 0.13317 | Test loss: 0.02160\n",
      "Epoch: 12940 | Train loss: 0.14376 | Test loss: 0.00180\n",
      "Epoch: 12950 | Train loss: 0.12425 | Test loss: 0.04205\n",
      "Epoch: 12960 | Train loss: 0.13484 | Test loss: 0.01776\n",
      "Epoch: 12970 | Train loss: 0.14485 | Test loss: 0.00382\n",
      "Epoch: 12980 | Train loss: 0.13949 | Test loss: 0.00710\n",
      "Epoch: 12990 | Train loss: 0.11929 | Test loss: 0.05186\n",
      "Epoch: 13000 | Train loss: 0.13058 | Test loss: 0.02755\n",
      "Epoch: 13010 | Train loss: 0.14117 | Test loss: 0.00325\n",
      "Epoch: 13020 | Train loss: 0.14285 | Test loss: 0.00152\n",
      "Epoch: 13030 | Train loss: 0.13749 | Test loss: 0.01169\n",
      "Epoch: 13040 | Train loss: 0.10930 | Test loss: 0.07486\n",
      "Epoch: 13050 | Train loss: 0.12264 | Test loss: 0.04576\n",
      "Epoch: 13060 | Train loss: 0.13323 | Test loss: 0.02146\n",
      "Epoch: 13070 | Train loss: 0.14383 | Test loss: 0.00187\n",
      "Epoch: 13080 | Train loss: 0.11769 | Test loss: 0.05553\n",
      "Epoch: 13090 | Train loss: 0.12898 | Test loss: 0.03123\n",
      "Epoch: 13100 | Train loss: 0.13957 | Test loss: 0.00693\n",
      "Epoch: 13110 | Train loss: 0.11936 | Test loss: 0.05168\n",
      "Epoch: 13120 | Train loss: 0.13065 | Test loss: 0.02738\n",
      "Epoch: 13130 | Train loss: 0.14125 | Test loss: 0.00308\n",
      "Epoch: 13140 | Train loss: 0.13530 | Test loss: 0.01672\n",
      "Epoch: 13150 | Train loss: 0.14531 | Test loss: 0.00485\n",
      "Epoch: 13160 | Train loss: 0.12638 | Test loss: 0.03718\n",
      "Epoch: 13170 | Train loss: 0.13698 | Test loss: 0.01288\n",
      "Epoch: 13180 | Train loss: 0.13103 | Test loss: 0.02652\n",
      "Epoch: 13190 | Train loss: 0.14162 | Test loss: 0.00225\n",
      "Epoch: 13200 | Train loss: 0.12212 | Test loss: 0.04698\n",
      "Epoch: 13210 | Train loss: 0.13271 | Test loss: 0.02267\n",
      "Epoch: 13220 | Train loss: 0.14330 | Test loss: 0.00146\n",
      "Epoch: 13230 | Train loss: 0.14116 | Test loss: 0.00329\n",
      "Epoch: 13240 | Train loss: 0.14284 | Test loss: 0.00150\n",
      "Epoch: 13250 | Train loss: 0.10269 | Test loss: 0.09012\n",
      "Epoch: 13260 | Train loss: 0.11671 | Test loss: 0.05781\n",
      "Epoch: 13270 | Train loss: 0.12869 | Test loss: 0.03191\n",
      "Epoch: 13280 | Train loss: 0.13928 | Test loss: 0.00761\n",
      "Epoch: 13290 | Train loss: 0.12580 | Test loss: 0.03853\n",
      "Epoch: 13300 | Train loss: 0.13639 | Test loss: 0.01423\n",
      "Epoch: 13310 | Train loss: 0.13104 | Test loss: 0.02651\n",
      "Epoch: 13320 | Train loss: 0.14163 | Test loss: 0.00224\n",
      "Epoch: 13330 | Train loss: 0.12213 | Test loss: 0.04696\n",
      "Epoch: 13340 | Train loss: 0.13272 | Test loss: 0.02266\n",
      "Epoch: 13350 | Train loss: 0.14331 | Test loss: 0.00146\n",
      "Epoch: 13360 | Train loss: 0.14117 | Test loss: 0.00328\n",
      "Epoch: 13370 | Train loss: 0.14285 | Test loss: 0.00150\n",
      "Epoch: 13380 | Train loss: 0.09922 | Test loss: 0.09636\n",
      "Epoch: 13390 | Train loss: 0.11401 | Test loss: 0.06405\n",
      "Epoch: 13400 | Train loss: 0.12667 | Test loss: 0.03654\n",
      "Epoch: 13410 | Train loss: 0.13726 | Test loss: 0.01224\n",
      "Epoch: 13420 | Train loss: 0.12379 | Test loss: 0.04316\n",
      "Epoch: 13430 | Train loss: 0.13438 | Test loss: 0.01886\n",
      "Epoch: 13440 | Train loss: 0.14438 | Test loss: 0.00274\n",
      "Epoch: 13450 | Train loss: 0.10280 | Test loss: 0.08989\n",
      "Epoch: 13460 | Train loss: 0.11682 | Test loss: 0.05758\n",
      "Epoch: 13470 | Train loss: 0.12880 | Test loss: 0.03168\n",
      "Epoch: 13480 | Train loss: 0.13939 | Test loss: 0.00738\n",
      "Epoch: 13490 | Train loss: 0.12591 | Test loss: 0.03830\n",
      "Epoch: 13500 | Train loss: 0.13650 | Test loss: 0.01400\n",
      "Epoch: 13510 | Train loss: 0.14592 | Test loss: 0.00622\n",
      "Epoch: 13520 | Train loss: 0.10376 | Test loss: 0.08767\n",
      "Epoch: 13530 | Train loss: 0.11778 | Test loss: 0.05536\n",
      "Epoch: 13540 | Train loss: 0.12907 | Test loss: 0.03106\n",
      "Epoch: 13550 | Train loss: 0.13966 | Test loss: 0.00676\n",
      "Epoch: 13560 | Train loss: 0.11216 | Test loss: 0.06833\n",
      "Epoch: 13570 | Train loss: 0.12481 | Test loss: 0.04083\n",
      "Epoch: 13580 | Train loss: 0.13540 | Test loss: 0.01652\n",
      "Epoch: 13590 | Train loss: 0.14541 | Test loss: 0.00505\n",
      "Epoch: 13600 | Train loss: 0.12055 | Test loss: 0.05060\n",
      "Epoch: 13610 | Train loss: 0.13115 | Test loss: 0.02630\n",
      "Epoch: 13620 | Train loss: 0.14174 | Test loss: 0.00207\n",
      "Epoch: 13630 | Train loss: 0.11492 | Test loss: 0.06197\n",
      "Epoch: 13640 | Train loss: 0.12689 | Test loss: 0.03607\n",
      "Epoch: 13650 | Train loss: 0.13748 | Test loss: 0.01177\n",
      "Epoch: 13660 | Train loss: 0.10929 | Test loss: 0.07495\n",
      "Epoch: 13670 | Train loss: 0.12263 | Test loss: 0.04584\n",
      "Epoch: 13680 | Train loss: 0.13322 | Test loss: 0.02154\n",
      "Epoch: 13690 | Train loss: 0.14382 | Test loss: 0.00183\n",
      "Epoch: 13700 | Train loss: 0.12431 | Test loss: 0.04199\n",
      "Epoch: 13710 | Train loss: 0.13490 | Test loss: 0.01769\n",
      "Epoch: 13720 | Train loss: 0.14491 | Test loss: 0.00389\n",
      "Epoch: 13730 | Train loss: 0.13955 | Test loss: 0.00703\n",
      "Epoch: 13740 | Train loss: 0.11934 | Test loss: 0.05179\n",
      "Epoch: 13750 | Train loss: 0.13063 | Test loss: 0.02748\n",
      "Epoch: 13760 | Train loss: 0.14123 | Test loss: 0.00319\n",
      "Epoch: 13770 | Train loss: 0.13528 | Test loss: 0.01683\n",
      "Epoch: 13780 | Train loss: 0.14529 | Test loss: 0.00475\n",
      "Epoch: 13790 | Train loss: 0.12636 | Test loss: 0.03728\n",
      "Epoch: 13800 | Train loss: 0.13696 | Test loss: 0.01298\n",
      "Epoch: 13810 | Train loss: 0.13863 | Test loss: 0.00913\n",
      "Epoch: 13820 | Train loss: 0.08860 | Test loss: 0.11896\n",
      "Epoch: 13830 | Train loss: 0.10658 | Test loss: 0.08120\n",
      "Epoch: 13840 | Train loss: 0.12060 | Test loss: 0.05050\n",
      "Epoch: 13850 | Train loss: 0.13120 | Test loss: 0.02619\n",
      "Epoch: 13860 | Train loss: 0.14179 | Test loss: 0.00200\n",
      "Epoch: 13870 | Train loss: 0.11497 | Test loss: 0.06187\n",
      "Epoch: 13880 | Train loss: 0.12694 | Test loss: 0.03596\n",
      "Epoch: 13890 | Train loss: 0.13753 | Test loss: 0.01166\n",
      "Epoch: 13900 | Train loss: 0.10934 | Test loss: 0.07484\n",
      "Epoch: 13910 | Train loss: 0.12268 | Test loss: 0.04574\n",
      "Epoch: 13920 | Train loss: 0.13328 | Test loss: 0.02144\n",
      "Epoch: 13930 | Train loss: 0.14387 | Test loss: 0.00188\n",
      "Epoch: 13940 | Train loss: 0.11773 | Test loss: 0.05550\n",
      "Epoch: 13950 | Train loss: 0.12902 | Test loss: 0.03120\n",
      "Epoch: 13960 | Train loss: 0.13961 | Test loss: 0.00690\n",
      "Epoch: 13970 | Train loss: 0.11210 | Test loss: 0.06848\n",
      "Epoch: 13980 | Train loss: 0.12476 | Test loss: 0.04098\n",
      "Epoch: 13990 | Train loss: 0.13535 | Test loss: 0.01668\n",
      "Epoch: 14000 | Train loss: 0.14536 | Test loss: 0.00490\n",
      "Epoch: 14010 | Train loss: 0.12644 | Test loss: 0.03713\n",
      "Epoch: 14020 | Train loss: 0.13703 | Test loss: 0.01283\n",
      "Epoch: 14030 | Train loss: 0.13108 | Test loss: 0.02647\n",
      "Epoch: 14040 | Train loss: 0.14168 | Test loss: 0.00220\n",
      "Epoch: 14050 | Train loss: 0.12217 | Test loss: 0.04692\n",
      "Epoch: 14060 | Train loss: 0.13276 | Test loss: 0.02262\n",
      "Epoch: 14070 | Train loss: 0.14336 | Test loss: 0.00146\n",
      "Epoch: 14080 | Train loss: 0.13741 | Test loss: 0.01197\n",
      "Epoch: 14090 | Train loss: 0.11652 | Test loss: 0.05832\n",
      "Epoch: 14100 | Train loss: 0.12849 | Test loss: 0.03242\n",
      "Epoch: 14110 | Train loss: 0.13909 | Test loss: 0.00812\n",
      "Epoch: 14120 | Train loss: 0.14076 | Test loss: 0.00427\n",
      "Epoch: 14130 | Train loss: 0.09306 | Test loss: 0.11068\n",
      "Epoch: 14140 | Train loss: 0.10939 | Test loss: 0.07474\n",
      "Epoch: 14150 | Train loss: 0.12274 | Test loss: 0.04563\n",
      "Epoch: 14160 | Train loss: 0.13333 | Test loss: 0.02133\n",
      "Epoch: 14170 | Train loss: 0.14393 | Test loss: 0.00194\n",
      "Epoch: 14180 | Train loss: 0.11779 | Test loss: 0.05540\n",
      "Epoch: 14190 | Train loss: 0.12907 | Test loss: 0.03110\n",
      "Epoch: 14200 | Train loss: 0.13966 | Test loss: 0.00680\n",
      "Epoch: 14210 | Train loss: 0.11216 | Test loss: 0.06838\n",
      "Epoch: 14220 | Train loss: 0.12481 | Test loss: 0.04087\n",
      "Epoch: 14230 | Train loss: 0.13541 | Test loss: 0.01657\n",
      "Epoch: 14240 | Train loss: 0.14542 | Test loss: 0.00500\n",
      "Epoch: 14250 | Train loss: 0.12055 | Test loss: 0.05064\n",
      "Epoch: 14260 | Train loss: 0.13115 | Test loss: 0.02634\n",
      "Epoch: 14270 | Train loss: 0.14174 | Test loss: 0.00211\n",
      "Epoch: 14280 | Train loss: 0.11492 | Test loss: 0.06202\n",
      "Epoch: 14290 | Train loss: 0.12689 | Test loss: 0.03611\n",
      "Epoch: 14300 | Train loss: 0.13748 | Test loss: 0.01182\n",
      "Epoch: 14310 | Train loss: 0.11659 | Test loss: 0.05817\n",
      "Epoch: 14320 | Train loss: 0.12787 | Test loss: 0.03387\n",
      "Epoch: 14330 | Train loss: 0.13846 | Test loss: 0.00957\n",
      "Epoch: 14340 | Train loss: 0.09540 | Test loss: 0.10527\n",
      "Epoch: 14350 | Train loss: 0.11096 | Test loss: 0.07114\n",
      "Epoch: 14360 | Train loss: 0.12361 | Test loss: 0.04364\n",
      "Epoch: 14370 | Train loss: 0.13421 | Test loss: 0.01934\n",
      "Epoch: 14380 | Train loss: 0.14421 | Test loss: 0.00236\n",
      "Epoch: 14390 | Train loss: 0.11086 | Test loss: 0.07138\n",
      "Epoch: 14400 | Train loss: 0.12351 | Test loss: 0.04387\n",
      "Epoch: 14410 | Train loss: 0.13410 | Test loss: 0.01957\n",
      "Epoch: 14420 | Train loss: 0.14411 | Test loss: 0.00219\n",
      "Epoch: 14430 | Train loss: 0.11076 | Test loss: 0.07162\n",
      "Epoch: 14440 | Train loss: 0.12411 | Test loss: 0.04251\n",
      "Epoch: 14450 | Train loss: 0.13470 | Test loss: 0.01821\n",
      "Epoch: 14460 | Train loss: 0.14471 | Test loss: 0.00337\n",
      "Epoch: 14470 | Train loss: 0.09450 | Test loss: 0.10736\n",
      "Epoch: 14480 | Train loss: 0.11084 | Test loss: 0.07142\n",
      "Epoch: 14490 | Train loss: 0.12349 | Test loss: 0.04392\n",
      "Epoch: 14500 | Train loss: 0.13409 | Test loss: 0.01962\n",
      "Epoch: 14510 | Train loss: 0.14410 | Test loss: 0.00215\n",
      "Epoch: 14520 | Train loss: 0.11074 | Test loss: 0.07166\n",
      "Epoch: 14530 | Train loss: 0.12409 | Test loss: 0.04255\n",
      "Epoch: 14540 | Train loss: 0.13468 | Test loss: 0.01825\n",
      "Epoch: 14550 | Train loss: 0.14469 | Test loss: 0.00332\n",
      "Epoch: 14560 | Train loss: 0.09448 | Test loss: 0.10741\n",
      "Epoch: 14570 | Train loss: 0.11083 | Test loss: 0.07147\n",
      "Epoch: 14580 | Train loss: 0.12348 | Test loss: 0.04396\n",
      "Epoch: 14590 | Train loss: 0.13407 | Test loss: 0.01966\n",
      "Epoch: 14600 | Train loss: 0.14408 | Test loss: 0.00213\n",
      "Epoch: 14610 | Train loss: 0.11073 | Test loss: 0.07170\n",
      "Epoch: 14620 | Train loss: 0.12338 | Test loss: 0.04420\n",
      "Epoch: 14630 | Train loss: 0.13397 | Test loss: 0.01990\n",
      "Epoch: 14640 | Train loss: 0.14398 | Test loss: 0.00198\n",
      "Epoch: 14650 | Train loss: 0.11783 | Test loss: 0.05533\n",
      "Epoch: 14660 | Train loss: 0.12912 | Test loss: 0.03103\n",
      "Epoch: 14670 | Train loss: 0.13971 | Test loss: 0.00673\n",
      "Epoch: 14680 | Train loss: 0.11220 | Test loss: 0.06830\n",
      "Epoch: 14690 | Train loss: 0.12486 | Test loss: 0.04080\n",
      "Epoch: 14700 | Train loss: 0.13545 | Test loss: 0.01650\n",
      "Epoch: 14710 | Train loss: 0.14547 | Test loss: 0.00507\n",
      "Epoch: 14720 | Train loss: 0.12060 | Test loss: 0.05057\n",
      "Epoch: 14730 | Train loss: 0.13120 | Test loss: 0.02627\n",
      "Epoch: 14740 | Train loss: 0.14179 | Test loss: 0.00205\n",
      "Epoch: 14750 | Train loss: 0.11497 | Test loss: 0.06195\n",
      "Epoch: 14760 | Train loss: 0.12694 | Test loss: 0.03604\n",
      "Epoch: 14770 | Train loss: 0.13753 | Test loss: 0.01174\n",
      "Epoch: 14780 | Train loss: 0.10934 | Test loss: 0.07492\n",
      "Epoch: 14790 | Train loss: 0.12268 | Test loss: 0.04581\n",
      "Epoch: 14800 | Train loss: 0.13327 | Test loss: 0.02151\n",
      "Epoch: 14810 | Train loss: 0.14387 | Test loss: 0.00184\n",
      "Epoch: 14820 | Train loss: 0.11773 | Test loss: 0.05558\n",
      "Epoch: 14830 | Train loss: 0.12902 | Test loss: 0.03128\n",
      "Epoch: 14840 | Train loss: 0.13961 | Test loss: 0.00698\n",
      "Epoch: 14850 | Train loss: 0.11940 | Test loss: 0.05334\n",
      "Epoch: 14860 | Train loss: 0.13000 | Test loss: 0.02904\n",
      "Epoch: 14870 | Train loss: 0.14059 | Test loss: 0.00474\n",
      "Epoch: 14880 | Train loss: 0.09898 | Test loss: 0.09702\n",
      "Epoch: 14890 | Train loss: 0.11377 | Test loss: 0.06471\n",
      "Epoch: 14900 | Train loss: 0.12574 | Test loss: 0.03881\n",
      "Epoch: 14910 | Train loss: 0.13633 | Test loss: 0.01450\n",
      "Epoch: 14920 | Train loss: 0.14575 | Test loss: 0.00571\n",
      "Epoch: 14930 | Train loss: 0.11367 | Test loss: 0.06495\n",
      "Epoch: 14940 | Train loss: 0.12564 | Test loss: 0.03905\n",
      "Epoch: 14950 | Train loss: 0.13623 | Test loss: 0.01474\n",
      "Epoch: 14960 | Train loss: 0.14565 | Test loss: 0.00547\n",
      "Epoch: 14970 | Train loss: 0.11357 | Test loss: 0.06518\n",
      "Epoch: 14980 | Train loss: 0.12553 | Test loss: 0.03928\n",
      "Epoch: 14990 | Train loss: 0.13613 | Test loss: 0.01498\n",
      "Epoch: 15000 | Train loss: 0.14555 | Test loss: 0.00524\n",
      "Epoch: 15010 | Train loss: 0.12068 | Test loss: 0.05041\n",
      "Epoch: 15020 | Train loss: 0.13128 | Test loss: 0.02611\n",
      "Epoch: 15030 | Train loss: 0.14187 | Test loss: 0.00194\n",
      "Epoch: 15040 | Train loss: 0.11505 | Test loss: 0.06179\n",
      "Epoch: 15050 | Train loss: 0.12702 | Test loss: 0.03588\n",
      "Epoch: 15060 | Train loss: 0.13761 | Test loss: 0.01158\n",
      "Epoch: 15070 | Train loss: 0.10942 | Test loss: 0.07476\n",
      "Epoch: 15080 | Train loss: 0.12276 | Test loss: 0.04565\n",
      "Epoch: 15090 | Train loss: 0.13335 | Test loss: 0.02135\n",
      "Epoch: 15100 | Train loss: 0.14395 | Test loss: 0.00192\n",
      "Epoch: 15110 | Train loss: 0.11781 | Test loss: 0.05543\n",
      "Epoch: 15120 | Train loss: 0.12910 | Test loss: 0.03112\n",
      "Epoch: 15130 | Train loss: 0.13969 | Test loss: 0.00682\n",
      "Epoch: 15140 | Train loss: 0.11218 | Test loss: 0.06840\n",
      "Epoch: 15150 | Train loss: 0.12484 | Test loss: 0.04089\n",
      "Epoch: 15160 | Train loss: 0.13543 | Test loss: 0.01659\n",
      "Epoch: 15170 | Train loss: 0.14544 | Test loss: 0.00498\n",
      "Epoch: 15180 | Train loss: 0.12058 | Test loss: 0.05066\n",
      "Epoch: 15190 | Train loss: 0.13117 | Test loss: 0.02636\n",
      "Epoch: 15200 | Train loss: 0.14177 | Test loss: 0.00212\n",
      "Epoch: 15210 | Train loss: 0.11494 | Test loss: 0.06204\n",
      "Epoch: 15220 | Train loss: 0.12692 | Test loss: 0.03613\n",
      "Epoch: 15230 | Train loss: 0.13751 | Test loss: 0.01183\n",
      "Epoch: 15240 | Train loss: 0.11661 | Test loss: 0.05820\n",
      "Epoch: 15250 | Train loss: 0.12790 | Test loss: 0.03389\n",
      "Epoch: 15260 | Train loss: 0.13849 | Test loss: 0.00959\n",
      "Epoch: 15270 | Train loss: 0.09542 | Test loss: 0.10529\n",
      "Epoch: 15280 | Train loss: 0.11099 | Test loss: 0.07116\n",
      "Epoch: 15290 | Train loss: 0.12364 | Test loss: 0.04366\n",
      "Epoch: 15300 | Train loss: 0.13423 | Test loss: 0.01936\n",
      "Epoch: 15310 | Train loss: 0.14424 | Test loss: 0.00234\n",
      "Epoch: 15320 | Train loss: 0.11088 | Test loss: 0.07140\n",
      "Epoch: 15330 | Train loss: 0.12354 | Test loss: 0.04389\n",
      "Epoch: 15340 | Train loss: 0.13413 | Test loss: 0.01959\n",
      "Epoch: 15350 | Train loss: 0.14414 | Test loss: 0.00217\n",
      "Epoch: 15360 | Train loss: 0.11078 | Test loss: 0.07163\n",
      "Epoch: 15370 | Train loss: 0.12343 | Test loss: 0.04413\n",
      "Epoch: 15380 | Train loss: 0.13403 | Test loss: 0.01983\n",
      "Epoch: 15390 | Train loss: 0.14404 | Test loss: 0.00202\n",
      "Epoch: 15400 | Train loss: 0.11789 | Test loss: 0.05526\n",
      "Epoch: 15410 | Train loss: 0.12918 | Test loss: 0.03097\n",
      "Epoch: 15420 | Train loss: 0.13977 | Test loss: 0.00666\n",
      "Epoch: 15430 | Train loss: 0.13323 | Test loss: 0.02167\n",
      "Epoch: 15440 | Train loss: 0.14323 | Test loss: 0.00141\n",
      "Epoch: 15450 | Train loss: 0.14432 | Test loss: 0.00247\n",
      "Epoch: 15460 | Train loss: 0.10544 | Test loss: 0.08396\n",
      "Epoch: 15470 | Train loss: 0.11946 | Test loss: 0.05325\n",
      "Epoch: 15480 | Train loss: 0.13005 | Test loss: 0.02895\n",
      "Epoch: 15490 | Train loss: 0.14065 | Test loss: 0.00465\n",
      "Epoch: 15500 | Train loss: 0.09903 | Test loss: 0.09694\n",
      "Epoch: 15510 | Train loss: 0.11383 | Test loss: 0.06463\n",
      "Epoch: 15520 | Train loss: 0.12580 | Test loss: 0.03872\n",
      "Epoch: 15530 | Train loss: 0.13639 | Test loss: 0.01443\n",
      "Epoch: 15540 | Train loss: 0.14581 | Test loss: 0.00579\n",
      "Epoch: 15550 | Train loss: 0.10820 | Test loss: 0.07760\n",
      "Epoch: 15560 | Train loss: 0.12154 | Test loss: 0.04849\n",
      "Epoch: 15570 | Train loss: 0.13213 | Test loss: 0.02419\n",
      "Epoch: 15580 | Train loss: 0.14273 | Test loss: 0.00163\n",
      "Epoch: 15590 | Train loss: 0.10257 | Test loss: 0.09057\n",
      "Epoch: 15600 | Train loss: 0.11659 | Test loss: 0.05826\n",
      "Epoch: 15610 | Train loss: 0.12788 | Test loss: 0.03396\n",
      "Epoch: 15620 | Train loss: 0.13847 | Test loss: 0.00966\n",
      "Epoch: 15630 | Train loss: 0.10247 | Test loss: 0.09081\n",
      "Epoch: 15640 | Train loss: 0.11649 | Test loss: 0.05850\n",
      "Epoch: 15650 | Train loss: 0.12777 | Test loss: 0.03420\n",
      "Epoch: 15660 | Train loss: 0.13837 | Test loss: 0.00990\n",
      "Epoch: 15670 | Train loss: 0.10237 | Test loss: 0.09104\n",
      "Epoch: 15680 | Train loss: 0.11639 | Test loss: 0.05874\n",
      "Epoch: 15690 | Train loss: 0.12767 | Test loss: 0.03443\n",
      "Epoch: 15700 | Train loss: 0.13827 | Test loss: 0.01013\n",
      "Epoch: 15710 | Train loss: 0.10948 | Test loss: 0.07467\n",
      "Epoch: 15720 | Train loss: 0.12282 | Test loss: 0.04557\n",
      "Epoch: 15730 | Train loss: 0.13341 | Test loss: 0.02127\n",
      "Epoch: 15740 | Train loss: 0.14342 | Test loss: 0.00145\n",
      "Epoch: 15750 | Train loss: 0.13747 | Test loss: 0.01197\n",
      "Epoch: 15760 | Train loss: 0.13855 | Test loss: 0.00949\n",
      "Epoch: 15770 | Train loss: 0.09548 | Test loss: 0.10519\n",
      "Epoch: 15780 | Train loss: 0.11105 | Test loss: 0.07106\n",
      "Epoch: 15790 | Train loss: 0.12370 | Test loss: 0.04356\n",
      "Epoch: 15800 | Train loss: 0.13429 | Test loss: 0.01926\n",
      "Epoch: 15810 | Train loss: 0.14430 | Test loss: 0.00241\n",
      "Epoch: 15820 | Train loss: 0.10542 | Test loss: 0.08404\n",
      "Epoch: 15830 | Train loss: 0.11944 | Test loss: 0.05333\n",
      "Epoch: 15840 | Train loss: 0.13004 | Test loss: 0.02902\n",
      "Epoch: 15850 | Train loss: 0.14063 | Test loss: 0.00473\n",
      "Epoch: 15860 | Train loss: 0.09901 | Test loss: 0.09701\n",
      "Epoch: 15870 | Train loss: 0.11381 | Test loss: 0.06470\n",
      "Epoch: 15880 | Train loss: 0.12578 | Test loss: 0.03880\n",
      "Epoch: 15890 | Train loss: 0.13637 | Test loss: 0.01449\n",
      "Epoch: 15900 | Train loss: 0.14580 | Test loss: 0.00572\n",
      "Epoch: 15910 | Train loss: 0.11095 | Test loss: 0.07129\n",
      "Epoch: 15920 | Train loss: 0.12361 | Test loss: 0.04378\n",
      "Epoch: 15930 | Train loss: 0.13420 | Test loss: 0.01949\n",
      "Epoch: 15940 | Train loss: 0.14421 | Test loss: 0.00225\n",
      "Epoch: 15950 | Train loss: 0.11085 | Test loss: 0.07153\n",
      "Epoch: 15960 | Train loss: 0.12350 | Test loss: 0.04402\n",
      "Epoch: 15970 | Train loss: 0.13410 | Test loss: 0.01972\n",
      "Epoch: 15980 | Train loss: 0.14411 | Test loss: 0.00209\n",
      "Epoch: 15990 | Train loss: 0.11075 | Test loss: 0.07176\n",
      "Epoch: 16000 | Train loss: 0.12340 | Test loss: 0.04426\n",
      "Epoch: 16010 | Train loss: 0.13400 | Test loss: 0.01996\n",
      "Epoch: 16020 | Train loss: 0.14401 | Test loss: 0.00194\n",
      "Epoch: 16030 | Train loss: 0.11786 | Test loss: 0.05539\n",
      "Epoch: 16040 | Train loss: 0.12914 | Test loss: 0.03109\n",
      "Epoch: 16050 | Train loss: 0.13974 | Test loss: 0.00679\n",
      "Epoch: 16060 | Train loss: 0.14082 | Test loss: 0.00431\n",
      "Epoch: 16070 | Train loss: 0.09223 | Test loss: 0.11274\n",
      "Epoch: 16080 | Train loss: 0.10857 | Test loss: 0.07680\n",
      "Epoch: 16090 | Train loss: 0.12191 | Test loss: 0.04769\n",
      "Epoch: 16100 | Train loss: 0.13250 | Test loss: 0.02339\n",
      "Epoch: 16110 | Train loss: 0.14310 | Test loss: 0.00142\n",
      "Epoch: 16120 | Train loss: 0.09596 | Test loss: 0.10411\n",
      "Epoch: 16130 | Train loss: 0.11152 | Test loss: 0.06999\n",
      "Epoch: 16140 | Train loss: 0.12418 | Test loss: 0.04248\n",
      "Epoch: 16150 | Train loss: 0.13477 | Test loss: 0.01818\n",
      "Epoch: 16160 | Train loss: 0.14479 | Test loss: 0.00340\n",
      "Epoch: 16170 | Train loss: 0.14588 | Test loss: 0.00588\n",
      "Epoch: 16180 | Train loss: 0.10826 | Test loss: 0.07751\n",
      "Epoch: 16190 | Train loss: 0.12160 | Test loss: 0.04840\n",
      "Epoch: 16200 | Train loss: 0.13219 | Test loss: 0.02411\n",
      "Epoch: 16210 | Train loss: 0.14279 | Test loss: 0.00160\n",
      "Epoch: 16220 | Train loss: 0.10263 | Test loss: 0.09049\n",
      "Epoch: 16230 | Train loss: 0.11665 | Test loss: 0.05817\n",
      "Epoch: 16240 | Train loss: 0.12794 | Test loss: 0.03387\n",
      "Epoch: 16250 | Train loss: 0.13853 | Test loss: 0.00958\n",
      "Epoch: 16260 | Train loss: 0.09546 | Test loss: 0.10528\n",
      "Epoch: 16270 | Train loss: 0.11103 | Test loss: 0.07115\n",
      "Epoch: 16280 | Train loss: 0.12368 | Test loss: 0.04364\n",
      "Epoch: 16290 | Train loss: 0.13427 | Test loss: 0.01934\n",
      "Epoch: 16300 | Train loss: 0.14428 | Test loss: 0.00235\n",
      "Epoch: 16310 | Train loss: 0.10540 | Test loss: 0.08412\n",
      "Epoch: 16320 | Train loss: 0.11942 | Test loss: 0.05341\n",
      "Epoch: 16330 | Train loss: 0.13002 | Test loss: 0.02911\n",
      "Epoch: 16340 | Train loss: 0.14061 | Test loss: 0.00481\n",
      "Epoch: 16350 | Train loss: 0.10530 | Test loss: 0.08436\n",
      "Epoch: 16360 | Train loss: 0.11932 | Test loss: 0.05365\n",
      "Epoch: 16370 | Train loss: 0.12991 | Test loss: 0.02935\n",
      "Epoch: 16380 | Train loss: 0.14051 | Test loss: 0.00505\n",
      "Epoch: 16390 | Train loss: 0.10519 | Test loss: 0.08460\n",
      "Epoch: 16400 | Train loss: 0.11922 | Test loss: 0.05389\n",
      "Epoch: 16410 | Train loss: 0.12981 | Test loss: 0.02959\n",
      "Epoch: 16420 | Train loss: 0.14041 | Test loss: 0.00529\n",
      "Epoch: 16430 | Train loss: 0.11230 | Test loss: 0.06822\n",
      "Epoch: 16440 | Train loss: 0.12496 | Test loss: 0.04072\n",
      "Epoch: 16450 | Train loss: 0.13555 | Test loss: 0.01642\n",
      "Epoch: 16460 | Train loss: 0.14498 | Test loss: 0.00380\n",
      "Epoch: 16470 | Train loss: 0.13961 | Test loss: 0.00712\n",
      "Epoch: 16480 | Train loss: 0.14069 | Test loss: 0.00464\n",
      "Epoch: 16490 | Train loss: 0.09907 | Test loss: 0.09692\n",
      "Epoch: 16500 | Train loss: 0.11387 | Test loss: 0.06461\n",
      "Epoch: 16510 | Train loss: 0.12584 | Test loss: 0.03871\n",
      "Epoch: 16520 | Train loss: 0.13643 | Test loss: 0.01440\n",
      "Epoch: 16530 | Train loss: 0.14586 | Test loss: 0.00581\n",
      "Epoch: 16540 | Train loss: 0.10824 | Test loss: 0.07759\n",
      "Epoch: 16550 | Train loss: 0.12158 | Test loss: 0.04848\n",
      "Epoch: 16560 | Train loss: 0.13218 | Test loss: 0.02418\n",
      "Epoch: 16570 | Train loss: 0.14278 | Test loss: 0.00163\n",
      "Epoch: 16580 | Train loss: 0.10261 | Test loss: 0.09056\n",
      "Epoch: 16590 | Train loss: 0.11663 | Test loss: 0.05825\n",
      "Epoch: 16600 | Train loss: 0.12792 | Test loss: 0.03395\n",
      "Epoch: 16610 | Train loss: 0.13851 | Test loss: 0.00965\n",
      "Epoch: 16620 | Train loss: 0.09898 | Test loss: 0.09715\n",
      "Epoch: 16630 | Train loss: 0.11378 | Test loss: 0.06484\n",
      "Epoch: 16640 | Train loss: 0.12574 | Test loss: 0.03893\n",
      "Epoch: 16650 | Train loss: 0.13634 | Test loss: 0.01464\n",
      "Epoch: 16660 | Train loss: 0.14576 | Test loss: 0.00558\n",
      "Epoch: 16670 | Train loss: 0.11367 | Test loss: 0.06508\n",
      "Epoch: 16680 | Train loss: 0.12564 | Test loss: 0.03917\n",
      "Epoch: 16690 | Train loss: 0.13624 | Test loss: 0.01487\n",
      "Epoch: 16700 | Train loss: 0.14566 | Test loss: 0.00534\n",
      "Epoch: 16710 | Train loss: 0.11357 | Test loss: 0.06531\n",
      "Epoch: 16720 | Train loss: 0.12554 | Test loss: 0.03941\n",
      "Epoch: 16730 | Train loss: 0.13614 | Test loss: 0.01511\n",
      "Epoch: 16740 | Train loss: 0.14556 | Test loss: 0.00511\n",
      "Epoch: 16750 | Train loss: 0.12069 | Test loss: 0.05054\n",
      "Epoch: 16760 | Train loss: 0.13128 | Test loss: 0.02624\n",
      "Epoch: 16770 | Train loss: 0.14188 | Test loss: 0.00330\n",
      "Epoch: 16780 | Train loss: 0.14297 | Test loss: 0.00148\n",
      "Epoch: 16790 | Train loss: 0.09582 | Test loss: 0.10448\n",
      "Epoch: 16800 | Train loss: 0.11139 | Test loss: 0.07035\n",
      "Epoch: 16810 | Train loss: 0.12405 | Test loss: 0.04285\n",
      "Epoch: 16820 | Train loss: 0.13464 | Test loss: 0.01854\n",
      "Epoch: 16830 | Train loss: 0.14406 | Test loss: 0.00196\n",
      "Epoch: 16840 | Train loss: 0.11790 | Test loss: 0.05534\n",
      "Epoch: 16850 | Train loss: 0.12919 | Test loss: 0.03104\n",
      "Epoch: 16860 | Train loss: 0.13979 | Test loss: 0.00674\n",
      "Epoch: 16870 | Train loss: 0.13325 | Test loss: 0.02174\n",
      "Epoch: 16880 | Train loss: 0.14326 | Test loss: 0.00140\n",
      "Epoch: 16890 | Train loss: 0.09099 | Test loss: 0.11568\n",
      "Epoch: 16900 | Train loss: 0.10732 | Test loss: 0.07974\n",
      "Epoch: 16910 | Train loss: 0.12066 | Test loss: 0.05063\n",
      "Epoch: 16920 | Train loss: 0.13125 | Test loss: 0.02633\n",
      "Epoch: 16930 | Train loss: 0.14185 | Test loss: 0.00339\n",
      "Epoch: 16940 | Train loss: 0.14293 | Test loss: 0.00151\n",
      "Epoch: 16950 | Train loss: 0.10277 | Test loss: 0.09023\n",
      "Epoch: 16960 | Train loss: 0.11679 | Test loss: 0.05792\n",
      "Epoch: 16970 | Train loss: 0.12808 | Test loss: 0.03362\n",
      "Epoch: 16980 | Train loss: 0.13867 | Test loss: 0.00932\n",
      "Epoch: 16990 | Train loss: 0.09559 | Test loss: 0.10502\n",
      "Epoch: 17000 | Train loss: 0.11116 | Test loss: 0.07089\n",
      "Epoch: 17010 | Train loss: 0.12382 | Test loss: 0.04339\n",
      "Epoch: 17020 | Train loss: 0.13441 | Test loss: 0.01909\n",
      "Epoch: 17030 | Train loss: 0.14443 | Test loss: 0.00255\n",
      "Epoch: 17040 | Train loss: 0.10553 | Test loss: 0.08387\n",
      "Epoch: 17050 | Train loss: 0.11956 | Test loss: 0.05316\n",
      "Epoch: 17060 | Train loss: 0.13015 | Test loss: 0.02886\n",
      "Epoch: 17070 | Train loss: 0.14075 | Test loss: 0.00456\n",
      "Epoch: 17080 | Train loss: 0.09913 | Test loss: 0.09684\n",
      "Epoch: 17090 | Train loss: 0.11393 | Test loss: 0.06453\n",
      "Epoch: 17100 | Train loss: 0.12590 | Test loss: 0.03863\n",
      "Epoch: 17110 | Train loss: 0.13649 | Test loss: 0.01432\n",
      "Epoch: 17120 | Train loss: 0.14592 | Test loss: 0.00589\n",
      "Epoch: 17130 | Train loss: 0.10830 | Test loss: 0.07750\n",
      "Epoch: 17140 | Train loss: 0.12164 | Test loss: 0.04840\n",
      "Epoch: 17150 | Train loss: 0.13223 | Test loss: 0.02410\n",
      "Epoch: 17160 | Train loss: 0.14283 | Test loss: 0.00159\n",
      "Epoch: 17170 | Train loss: 0.10267 | Test loss: 0.09048\n",
      "Epoch: 17180 | Train loss: 0.11669 | Test loss: 0.05816\n",
      "Epoch: 17190 | Train loss: 0.12798 | Test loss: 0.03387\n",
      "Epoch: 17200 | Train loss: 0.13857 | Test loss: 0.00956\n",
      "Epoch: 17210 | Train loss: 0.09549 | Test loss: 0.10527\n",
      "Epoch: 17220 | Train loss: 0.11106 | Test loss: 0.07114\n",
      "Epoch: 17230 | Train loss: 0.12372 | Test loss: 0.04364\n",
      "Epoch: 17240 | Train loss: 0.13431 | Test loss: 0.01933\n",
      "Epoch: 17250 | Train loss: 0.14433 | Test loss: 0.00235\n",
      "Epoch: 17260 | Train loss: 0.10543 | Test loss: 0.08411\n",
      "Epoch: 17270 | Train loss: 0.11946 | Test loss: 0.05340\n",
      "Epoch: 17280 | Train loss: 0.13006 | Test loss: 0.02910\n",
      "Epoch: 17290 | Train loss: 0.14065 | Test loss: 0.00480\n",
      "Epoch: 17300 | Train loss: 0.10258 | Test loss: 0.09070\n",
      "Epoch: 17310 | Train loss: 0.11660 | Test loss: 0.05839\n",
      "Epoch: 17320 | Train loss: 0.12788 | Test loss: 0.03410\n",
      "Epoch: 17330 | Train loss: 0.13848 | Test loss: 0.00979\n",
      "Epoch: 17340 | Train loss: 0.10247 | Test loss: 0.09094\n",
      "Epoch: 17350 | Train loss: 0.11650 | Test loss: 0.05863\n",
      "Epoch: 17360 | Train loss: 0.12778 | Test loss: 0.03433\n",
      "Epoch: 17370 | Train loss: 0.13837 | Test loss: 0.01003\n",
      "Epoch: 17380 | Train loss: 0.10237 | Test loss: 0.09118\n",
      "Epoch: 17390 | Train loss: 0.11639 | Test loss: 0.05887\n",
      "Epoch: 17400 | Train loss: 0.12768 | Test loss: 0.03457\n",
      "Epoch: 17410 | Train loss: 0.13827 | Test loss: 0.01027\n",
      "Epoch: 17420 | Train loss: 0.10948 | Test loss: 0.07481\n",
      "Epoch: 17430 | Train loss: 0.12248 | Test loss: 0.04650\n",
      "Epoch: 17440 | Train loss: 0.13307 | Test loss: 0.02220\n",
      "Epoch: 17450 | Train loss: 0.14308 | Test loss: 0.00144\n",
      "Epoch: 17460 | Train loss: 0.09593 | Test loss: 0.10428\n",
      "Epoch: 17470 | Train loss: 0.11150 | Test loss: 0.07015\n",
      "Epoch: 17480 | Train loss: 0.12416 | Test loss: 0.04265\n",
      "Epoch: 17490 | Train loss: 0.13475 | Test loss: 0.01835\n",
      "Epoch: 17500 | Train loss: 0.14417 | Test loss: 0.00209\n",
      "Epoch: 17510 | Train loss: 0.11081 | Test loss: 0.07176\n",
      "Epoch: 17520 | Train loss: 0.12346 | Test loss: 0.04424\n",
      "Epoch: 17530 | Train loss: 0.13406 | Test loss: 0.01994\n",
      "Epoch: 17540 | Train loss: 0.14407 | Test loss: 0.00194\n",
      "Epoch: 17550 | Train loss: 0.11791 | Test loss: 0.05698\n",
      "Epoch: 17560 | Train loss: 0.12850 | Test loss: 0.03268\n",
      "Epoch: 17570 | Train loss: 0.13910 | Test loss: 0.00838\n",
      "Epoch: 17580 | Train loss: 0.10250 | Test loss: 0.09089\n",
      "Epoch: 17590 | Train loss: 0.11652 | Test loss: 0.05858\n",
      "Epoch: 17600 | Train loss: 0.12781 | Test loss: 0.03428\n",
      "Epoch: 17610 | Train loss: 0.13840 | Test loss: 0.00998\n",
      "Epoch: 17620 | Train loss: 0.10240 | Test loss: 0.09113\n",
      "Epoch: 17630 | Train loss: 0.11642 | Test loss: 0.05882\n",
      "Epoch: 17640 | Train loss: 0.12771 | Test loss: 0.03452\n",
      "Epoch: 17650 | Train loss: 0.13830 | Test loss: 0.01022\n",
      "Epoch: 17660 | Train loss: 0.10951 | Test loss: 0.07476\n",
      "Epoch: 17670 | Train loss: 0.12216 | Test loss: 0.04725\n",
      "Epoch: 17680 | Train loss: 0.13275 | Test loss: 0.02295\n",
      "Epoch: 17690 | Train loss: 0.14276 | Test loss: 0.00168\n",
      "Epoch: 17700 | Train loss: 0.10812 | Test loss: 0.07796\n",
      "Epoch: 17710 | Train loss: 0.12146 | Test loss: 0.04885\n",
      "Epoch: 17720 | Train loss: 0.13206 | Test loss: 0.02455\n",
      "Epoch: 17730 | Train loss: 0.14206 | Test loss: 0.00297\n",
      "Epoch: 17740 | Train loss: 0.13552 | Test loss: 0.01662\n",
      "Epoch: 17750 | Train loss: 0.14494 | Test loss: 0.00360\n",
      "Epoch: 17760 | Train loss: 0.14543 | Test loss: 0.00472\n",
      "Epoch: 17770 | Train loss: 0.12580 | Test loss: 0.03892\n",
      "Epoch: 17780 | Train loss: 0.13639 | Test loss: 0.01461\n",
      "Epoch: 17790 | Train loss: 0.14522 | Test loss: 0.00424\n",
      "Epoch: 17800 | Train loss: 0.13232 | Test loss: 0.02395\n",
      "Epoch: 17810 | Train loss: 0.14233 | Test loss: 0.00238\n",
      "Epoch: 17820 | Train loss: 0.12152 | Test loss: 0.04873\n",
      "Epoch: 17830 | Train loss: 0.13211 | Test loss: 0.02443\n",
      "Epoch: 17840 | Train loss: 0.14212 | Test loss: 0.00285\n",
      "Epoch: 17850 | Train loss: 0.12804 | Test loss: 0.03378\n",
      "Epoch: 17860 | Train loss: 0.13863 | Test loss: 0.00948\n",
      "Epoch: 17870 | Train loss: 0.11655 | Test loss: 0.05855\n",
      "Epoch: 17880 | Train loss: 0.12783 | Test loss: 0.03425\n",
      "Epoch: 17890 | Train loss: 0.13842 | Test loss: 0.00995\n",
      "Epoch: 17900 | Train loss: 0.13129 | Test loss: 0.02632\n",
      "Epoch: 17910 | Train loss: 0.14189 | Test loss: 0.00338\n",
      "Epoch: 17920 | Train loss: 0.14238 | Test loss: 0.00228\n",
      "Epoch: 17930 | Train loss: 0.12157 | Test loss: 0.04862\n",
      "Epoch: 17940 | Train loss: 0.13217 | Test loss: 0.02432\n",
      "Epoch: 17950 | Train loss: 0.14217 | Test loss: 0.00274\n",
      "Epoch: 17960 | Train loss: 0.12809 | Test loss: 0.03366\n",
      "Epoch: 17970 | Train loss: 0.13869 | Test loss: 0.00936\n",
      "Epoch: 17980 | Train loss: 0.11660 | Test loss: 0.05844\n",
      "Epoch: 17990 | Train loss: 0.12789 | Test loss: 0.03414\n",
      "Epoch: 18000 | Train loss: 0.13848 | Test loss: 0.00984\n",
      "Epoch: 18010 | Train loss: 0.12381 | Test loss: 0.04348\n",
      "Epoch: 18020 | Train loss: 0.13441 | Test loss: 0.01918\n",
      "Epoch: 18030 | Train loss: 0.14383 | Test loss: 0.00164\n",
      "Epoch: 18040 | Train loss: 0.12361 | Test loss: 0.04396\n",
      "Epoch: 18050 | Train loss: 0.13420 | Test loss: 0.01966\n",
      "Epoch: 18060 | Train loss: 0.14362 | Test loss: 0.00149\n",
      "Epoch: 18070 | Train loss: 0.13766 | Test loss: 0.01173\n",
      "Epoch: 18080 | Train loss: 0.13874 | Test loss: 0.00924\n",
      "Epoch: 18090 | Train loss: 0.11665 | Test loss: 0.05832\n",
      "Epoch: 18100 | Train loss: 0.12794 | Test loss: 0.03402\n",
      "Epoch: 18110 | Train loss: 0.13853 | Test loss: 0.00972\n",
      "Epoch: 18120 | Train loss: 0.12387 | Test loss: 0.04337\n",
      "Epoch: 18130 | Train loss: 0.13446 | Test loss: 0.01907\n",
      "Epoch: 18140 | Train loss: 0.14388 | Test loss: 0.00169\n",
      "Epoch: 18150 | Train loss: 0.12366 | Test loss: 0.04384\n",
      "Epoch: 18160 | Train loss: 0.13425 | Test loss: 0.01954\n",
      "Epoch: 18170 | Train loss: 0.14367 | Test loss: 0.00152\n",
      "Epoch: 18180 | Train loss: 0.13018 | Test loss: 0.02889\n",
      "Epoch: 18190 | Train loss: 0.14077 | Test loss: 0.00594\n",
      "Epoch: 18200 | Train loss: 0.11938 | Test loss: 0.05367\n",
      "Epoch: 18210 | Train loss: 0.12997 | Test loss: 0.02936\n",
      "Epoch: 18220 | Train loss: 0.14057 | Test loss: 0.00643\n",
      "Epoch: 18230 | Train loss: 0.12966 | Test loss: 0.03009\n",
      "Epoch: 18240 | Train loss: 0.14025 | Test loss: 0.00715\n",
      "Epoch: 18250 | Train loss: 0.14075 | Test loss: 0.00603\n",
      "Epoch: 18260 | Train loss: 0.11935 | Test loss: 0.05374\n",
      "Epoch: 18270 | Train loss: 0.12994 | Test loss: 0.02944\n",
      "Epoch: 18280 | Train loss: 0.14054 | Test loss: 0.00650\n",
      "Epoch: 18290 | Train loss: 0.13340 | Test loss: 0.02151\n",
      "Epoch: 18300 | Train loss: 0.14341 | Test loss: 0.00141\n",
      "Epoch: 18310 | Train loss: 0.14390 | Test loss: 0.00171\n",
      "Epoch: 18320 | Train loss: 0.12368 | Test loss: 0.04380\n",
      "Epoch: 18330 | Train loss: 0.13428 | Test loss: 0.01950\n",
      "Epoch: 18340 | Train loss: 0.14370 | Test loss: 0.00153\n",
      "Epoch: 18350 | Train loss: 0.13020 | Test loss: 0.02885\n",
      "Epoch: 18360 | Train loss: 0.14080 | Test loss: 0.00591\n",
      "Epoch: 18370 | Train loss: 0.11940 | Test loss: 0.05363\n",
      "Epoch: 18380 | Train loss: 0.13000 | Test loss: 0.02933\n",
      "Epoch: 18390 | Train loss: 0.14059 | Test loss: 0.00639\n",
      "Epoch: 18400 | Train loss: 0.12592 | Test loss: 0.03867\n",
      "Epoch: 18410 | Train loss: 0.13652 | Test loss: 0.01437\n",
      "Epoch: 18420 | Train loss: 0.14535 | Test loss: 0.00448\n",
      "Epoch: 18430 | Train loss: 0.12572 | Test loss: 0.03915\n",
      "Epoch: 18440 | Train loss: 0.13631 | Test loss: 0.01485\n",
      "Epoch: 18450 | Train loss: 0.14514 | Test loss: 0.00401\n",
      "Epoch: 18460 | Train loss: 0.13977 | Test loss: 0.00692\n",
      "Epoch: 18470 | Train loss: 0.14086 | Test loss: 0.00579\n",
      "Epoch: 18480 | Train loss: 0.11946 | Test loss: 0.05351\n",
      "Epoch: 18490 | Train loss: 0.13005 | Test loss: 0.02921\n",
      "Epoch: 18500 | Train loss: 0.14065 | Test loss: 0.00627\n",
      "Epoch: 18510 | Train loss: 0.12598 | Test loss: 0.03855\n",
      "Epoch: 18520 | Train loss: 0.13657 | Test loss: 0.01425\n",
      "Epoch: 18530 | Train loss: 0.14541 | Test loss: 0.00460\n",
      "Epoch: 18540 | Train loss: 0.12577 | Test loss: 0.03903\n",
      "Epoch: 18550 | Train loss: 0.13636 | Test loss: 0.01473\n",
      "Epoch: 18560 | Train loss: 0.14520 | Test loss: 0.00412\n",
      "Epoch: 18570 | Train loss: 0.13229 | Test loss: 0.02407\n",
      "Epoch: 18580 | Train loss: 0.14230 | Test loss: 0.00250\n",
      "Epoch: 18590 | Train loss: 0.12149 | Test loss: 0.04885\n",
      "Epoch: 18600 | Train loss: 0.13209 | Test loss: 0.02455\n",
      "Epoch: 18610 | Train loss: 0.14209 | Test loss: 0.00298\n",
      "Epoch: 18620 | Train loss: 0.13554 | Test loss: 0.01662\n",
      "Epoch: 18630 | Train loss: 0.14497 | Test loss: 0.00360\n",
      "Epoch: 18640 | Train loss: 0.14546 | Test loss: 0.00472\n",
      "Epoch: 18650 | Train loss: 0.12583 | Test loss: 0.03892\n",
      "Epoch: 18660 | Train loss: 0.13642 | Test loss: 0.01461\n",
      "Epoch: 18670 | Train loss: 0.14525 | Test loss: 0.00424\n",
      "Epoch: 18680 | Train loss: 0.13235 | Test loss: 0.02396\n",
      "Epoch: 18690 | Train loss: 0.14236 | Test loss: 0.00238\n",
      "Epoch: 18700 | Train loss: 0.12155 | Test loss: 0.04874\n",
      "Epoch: 18710 | Train loss: 0.13214 | Test loss: 0.02444\n",
      "Epoch: 18720 | Train loss: 0.14215 | Test loss: 0.00286\n",
      "Epoch: 18730 | Train loss: 0.12807 | Test loss: 0.03378\n",
      "Epoch: 18740 | Train loss: 0.13866 | Test loss: 0.00948\n",
      "Epoch: 18750 | Train loss: 0.11657 | Test loss: 0.05856\n",
      "Epoch: 18760 | Train loss: 0.12786 | Test loss: 0.03426\n",
      "Epoch: 18770 | Train loss: 0.13845 | Test loss: 0.00996\n",
      "Epoch: 18780 | Train loss: 0.13132 | Test loss: 0.02633\n",
      "Epoch: 18790 | Train loss: 0.14192 | Test loss: 0.00339\n",
      "Epoch: 18800 | Train loss: 0.14241 | Test loss: 0.00228\n",
      "Epoch: 18810 | Train loss: 0.12160 | Test loss: 0.04862\n",
      "Epoch: 18820 | Train loss: 0.13219 | Test loss: 0.02432\n",
      "Epoch: 18830 | Train loss: 0.14220 | Test loss: 0.00274\n",
      "Epoch: 18840 | Train loss: 0.12812 | Test loss: 0.03367\n",
      "Epoch: 18850 | Train loss: 0.13871 | Test loss: 0.00936\n",
      "Epoch: 18860 | Train loss: 0.11663 | Test loss: 0.05844\n",
      "Epoch: 18870 | Train loss: 0.12791 | Test loss: 0.03414\n",
      "Epoch: 18880 | Train loss: 0.13851 | Test loss: 0.00984\n",
      "Epoch: 18890 | Train loss: 0.12384 | Test loss: 0.04349\n",
      "Epoch: 18900 | Train loss: 0.13443 | Test loss: 0.01918\n",
      "Epoch: 18910 | Train loss: 0.14386 | Test loss: 0.00163\n",
      "Epoch: 18920 | Train loss: 0.12363 | Test loss: 0.04396\n",
      "Epoch: 18930 | Train loss: 0.13423 | Test loss: 0.01966\n",
      "Epoch: 18940 | Train loss: 0.14365 | Test loss: 0.00148\n",
      "Epoch: 18950 | Train loss: 0.13769 | Test loss: 0.01173\n",
      "Epoch: 18960 | Train loss: 0.13877 | Test loss: 0.00925\n",
      "Epoch: 18970 | Train loss: 0.11668 | Test loss: 0.05833\n",
      "Epoch: 18980 | Train loss: 0.12797 | Test loss: 0.03403\n",
      "Epoch: 18990 | Train loss: 0.13856 | Test loss: 0.00972\n",
      "Epoch: 19000 | Train loss: 0.12390 | Test loss: 0.04337\n",
      "Epoch: 19010 | Train loss: 0.13449 | Test loss: 0.01907\n",
      "Epoch: 19020 | Train loss: 0.14391 | Test loss: 0.00169\n",
      "Epoch: 19030 | Train loss: 0.12369 | Test loss: 0.04385\n",
      "Epoch: 19040 | Train loss: 0.13428 | Test loss: 0.01955\n",
      "Epoch: 19050 | Train loss: 0.14370 | Test loss: 0.00151\n",
      "Epoch: 19060 | Train loss: 0.13021 | Test loss: 0.02889\n",
      "Epoch: 19070 | Train loss: 0.14081 | Test loss: 0.00596\n",
      "Epoch: 19080 | Train loss: 0.11941 | Test loss: 0.05367\n",
      "Epoch: 19090 | Train loss: 0.13000 | Test loss: 0.02937\n",
      "Epoch: 19100 | Train loss: 0.14060 | Test loss: 0.00643\n",
      "Epoch: 19110 | Train loss: 0.12969 | Test loss: 0.03009\n",
      "Epoch: 19120 | Train loss: 0.14028 | Test loss: 0.00715\n",
      "Epoch: 19130 | Train loss: 0.14078 | Test loss: 0.00602\n",
      "Epoch: 19140 | Train loss: 0.11938 | Test loss: 0.05374\n",
      "Epoch: 19150 | Train loss: 0.12997 | Test loss: 0.02944\n",
      "Epoch: 19160 | Train loss: 0.14057 | Test loss: 0.00651\n",
      "Epoch: 19170 | Train loss: 0.13343 | Test loss: 0.02151\n",
      "Epoch: 19180 | Train loss: 0.14285 | Test loss: 0.00164\n",
      "Epoch: 19190 | Train loss: 0.10267 | Test loss: 0.09061\n",
      "Epoch: 19200 | Train loss: 0.11670 | Test loss: 0.05830\n",
      "Epoch: 19210 | Train loss: 0.12799 | Test loss: 0.03400\n",
      "Epoch: 19220 | Train loss: 0.13858 | Test loss: 0.00971\n",
      "Epoch: 19230 | Train loss: 0.12391 | Test loss: 0.04335\n",
      "Epoch: 19240 | Train loss: 0.13451 | Test loss: 0.01905\n",
      "Epoch: 19250 | Train loss: 0.14393 | Test loss: 0.00170\n",
      "Epoch: 19260 | Train loss: 0.12371 | Test loss: 0.04383\n",
      "Epoch: 19270 | Train loss: 0.13430 | Test loss: 0.01953\n",
      "Epoch: 19280 | Train loss: 0.14372 | Test loss: 0.00152\n",
      "Epoch: 19290 | Train loss: 0.13023 | Test loss: 0.02887\n",
      "Epoch: 19300 | Train loss: 0.14082 | Test loss: 0.00594\n",
      "Epoch: 19310 | Train loss: 0.11942 | Test loss: 0.05365\n",
      "Epoch: 19320 | Train loss: 0.13002 | Test loss: 0.02935\n",
      "Epoch: 19330 | Train loss: 0.14061 | Test loss: 0.00641\n",
      "Epoch: 19340 | Train loss: 0.12595 | Test loss: 0.03868\n",
      "Epoch: 19350 | Train loss: 0.13654 | Test loss: 0.01439\n",
      "Epoch: 19360 | Train loss: 0.14537 | Test loss: 0.00446\n",
      "Epoch: 19370 | Train loss: 0.12574 | Test loss: 0.03917\n",
      "Epoch: 19380 | Train loss: 0.13633 | Test loss: 0.01487\n",
      "Epoch: 19390 | Train loss: 0.14517 | Test loss: 0.00398\n",
      "Epoch: 19400 | Train loss: 0.13979 | Test loss: 0.00830\n",
      "Epoch: 19410 | Train loss: 0.09552 | Test loss: 0.10537\n",
      "Epoch: 19420 | Train loss: 0.11109 | Test loss: 0.07124\n",
      "Epoch: 19430 | Train loss: 0.12375 | Test loss: 0.04373\n",
      "Epoch: 19440 | Train loss: 0.13435 | Test loss: 0.01943\n",
      "Epoch: 19450 | Train loss: 0.14377 | Test loss: 0.00154\n",
      "Epoch: 19460 | Train loss: 0.13027 | Test loss: 0.02878\n",
      "Epoch: 19470 | Train loss: 0.14087 | Test loss: 0.00584\n",
      "Epoch: 19480 | Train loss: 0.11947 | Test loss: 0.05356\n",
      "Epoch: 19490 | Train loss: 0.13007 | Test loss: 0.02925\n",
      "Epoch: 19500 | Train loss: 0.14066 | Test loss: 0.00632\n",
      "Epoch: 19510 | Train loss: 0.12599 | Test loss: 0.03859\n",
      "Epoch: 19520 | Train loss: 0.13659 | Test loss: 0.01430\n",
      "Epoch: 19530 | Train loss: 0.14542 | Test loss: 0.00455\n",
      "Epoch: 19540 | Train loss: 0.12579 | Test loss: 0.03908\n",
      "Epoch: 19550 | Train loss: 0.13638 | Test loss: 0.01478\n",
      "Epoch: 19560 | Train loss: 0.14521 | Test loss: 0.00408\n",
      "Epoch: 19570 | Train loss: 0.13231 | Test loss: 0.02412\n",
      "Epoch: 19580 | Train loss: 0.14232 | Test loss: 0.00254\n",
      "Epoch: 19590 | Train loss: 0.12151 | Test loss: 0.04890\n",
      "Epoch: 19600 | Train loss: 0.13210 | Test loss: 0.02460\n",
      "Epoch: 19610 | Train loss: 0.14211 | Test loss: 0.00302\n",
      "Epoch: 19620 | Train loss: 0.13556 | Test loss: 0.01667\n",
      "Epoch: 19630 | Train loss: 0.14439 | Test loss: 0.00231\n",
      "Epoch: 19640 | Train loss: 0.10549 | Test loss: 0.08417\n",
      "Epoch: 19650 | Train loss: 0.11952 | Test loss: 0.05346\n",
      "Epoch: 19660 | Train loss: 0.13011 | Test loss: 0.02916\n",
      "Epoch: 19670 | Train loss: 0.14071 | Test loss: 0.00622\n",
      "Epoch: 19680 | Train loss: 0.12604 | Test loss: 0.03850\n",
      "Epoch: 19690 | Train loss: 0.13663 | Test loss: 0.01420\n",
      "Epoch: 19700 | Train loss: 0.14547 | Test loss: 0.00465\n",
      "Epoch: 19710 | Train loss: 0.12583 | Test loss: 0.03898\n",
      "Epoch: 19720 | Train loss: 0.13643 | Test loss: 0.01468\n",
      "Epoch: 19730 | Train loss: 0.14526 | Test loss: 0.00417\n",
      "Epoch: 19740 | Train loss: 0.13235 | Test loss: 0.02403\n",
      "Epoch: 19750 | Train loss: 0.14236 | Test loss: 0.00245\n",
      "Epoch: 19760 | Train loss: 0.12155 | Test loss: 0.04880\n",
      "Epoch: 19770 | Train loss: 0.13215 | Test loss: 0.02450\n",
      "Epoch: 19780 | Train loss: 0.14216 | Test loss: 0.00293\n",
      "Epoch: 19790 | Train loss: 0.12807 | Test loss: 0.03385\n",
      "Epoch: 19800 | Train loss: 0.13867 | Test loss: 0.00955\n",
      "Epoch: 19810 | Train loss: 0.11658 | Test loss: 0.05863\n",
      "Epoch: 19820 | Train loss: 0.12787 | Test loss: 0.03432\n",
      "Epoch: 19830 | Train loss: 0.13846 | Test loss: 0.01002\n",
      "Epoch: 19840 | Train loss: 0.13132 | Test loss: 0.02640\n",
      "Epoch: 19850 | Train loss: 0.14133 | Test loss: 0.00482\n",
      "Epoch: 19860 | Train loss: 0.09910 | Test loss: 0.09710\n",
      "Epoch: 19870 | Train loss: 0.11390 | Test loss: 0.06479\n",
      "Epoch: 19880 | Train loss: 0.12588 | Test loss: 0.03888\n",
      "Epoch: 19890 | Train loss: 0.13647 | Test loss: 0.01459\n",
      "Epoch: 19900 | Train loss: 0.14531 | Test loss: 0.00427\n",
      "Epoch: 19910 | Train loss: 0.13240 | Test loss: 0.02393\n",
      "Epoch: 19920 | Train loss: 0.14241 | Test loss: 0.00236\n",
      "Epoch: 19930 | Train loss: 0.12160 | Test loss: 0.04871\n",
      "Epoch: 19940 | Train loss: 0.13219 | Test loss: 0.02441\n",
      "Epoch: 19950 | Train loss: 0.14220 | Test loss: 0.00283\n",
      "Epoch: 19960 | Train loss: 0.12812 | Test loss: 0.03375\n",
      "Epoch: 19970 | Train loss: 0.13871 | Test loss: 0.00945\n",
      "Epoch: 19980 | Train loss: 0.11662 | Test loss: 0.05853\n",
      "Epoch: 19990 | Train loss: 0.12791 | Test loss: 0.03423\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs=20000\n",
    "x_train=x_train.to(device)\n",
    "y_train=y_train.to(device)\n",
    "x_test=x_test.to(device)\n",
    "y_test=y_test.to(device)\n",
    "for epoch in range(epochs):\n",
    "    model_1.train()\n",
    "    y_pred=model_1(x_train)\n",
    "    loss=loss_fn(y_pred,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_test_pred=model_1(x_test)\n",
    "        test_loss=loss_fn(y_test_pred,y_test)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Train loss: {loss:.5f} | Test loss: {test_loss:.5f}\")\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.scatter(x_train.cpu(),y_train.cpu(),c=\"blue\",s=4,label=\"train\")\n",
    "plt.scatter(x_test.cpu(),y_test.cpu(),c=\"orange\",s=4,label=\"test\")\n",
    "plt.scatter(x_test.cpu(),y_test_pred.cpu(),c=\"red\",s=4,label=\"pred\")\n",
    "plt.legend(prop={\"size\":15})\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
